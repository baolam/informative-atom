{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46d6f92c",
   "metadata": {},
   "source": [
    "C√°ch thi·∫øt k·∫ø ·ªü d∆∞·ªõi ƒë√¢y l√† kh√¥ng ph√π h·ª£p, c√°c th√†nh ph·∫ßn tr·ªè qua l·∫°i l·∫´n nhau l√†m tƒÉng m·ªëi quan h·ªá ph·ª• thu·ªôc c√°c th√†nh ph·∫ßn. H·ªá qu·∫£, k·∫øt qu·∫£ h·ªçc ·ªü ƒë∆°n v·ªã n√†y d·∫´n ƒë·∫øn tri·ªát ti√™u k·∫øt qu·∫£ ·ªü ƒë∆°n v·ªã kh√°c. Ngo√†i ra c√¥ng th·ª©c to√°n c√†i ƒë·∫∑t ch∆∞a t·ªët. M√¥ h√¨nh ch∆∞a th·ªÉ t·ªëi ∆∞u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "385c52da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from fgi import *\n",
    "from torch import jit\n",
    "from torch import nn, randn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning import LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0f92faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.cuda import is_available\n",
    "is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a07e3e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Digit:\n",
    "    def __init__(self, digit ,*args, **kwargs):\n",
    "        self._digit = int(digit)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Digit({self._digit})\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67808e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitProblem(NonCodeProblem):\n",
    "    \"\"\"\n",
    "    B·ªô ph√¢n lo·∫°i ch·ªØ s·ªë\n",
    "    \"\"\"\n",
    "    def __init__(self, _id, *args, **kwargs):\n",
    "        super().__init__(_id, *args, **kwargs)\n",
    "        self._represent = RepresentLayer.from_units([\n",
    "            ImageRepresent(img_shape=(1, 28, 28), patch_size=4, num_heads=1, phi_dim=128),\n",
    "            ImageRepresent(img_shape=(1, 28, 28), patch_size=4, num_heads=1, phi_dim=128),\n",
    "            # EdgeRepresent(img_shape=(1, 28, 28), patch_size=4, num_heads=2, phi_dim=128)\n",
    "        ], output_dim=128)\n",
    "        self._co_represent = CoRepresentLayer.from_units(\n",
    "            [ CoRepresentUnit(2, phi_dim=128) for _ in range(2) ]\n",
    "        )\n",
    "        self._property = PropertyLayer.from_units(\n",
    "            [ PropertyUnit(phi_dim=128) for _ in range(4) ]\n",
    "        )\n",
    "        self._co_property = CoPropertyLayer.from_units(\n",
    "            [ ChooseOptions(4, options=[\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"], property_name=\"digit\", phi_dim=128) ]\n",
    "        )\n",
    "\n",
    "        self._update_additional_infor()\n",
    "    \n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        x = self._represent(x)\n",
    "        x = self._co_represent(x)\n",
    "        x = self._property(x)\n",
    "        x = self._co_property(x)\n",
    "        return x\n",
    "    \n",
    "    def recognize_unknown(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def _as_object(self):\n",
    "        return Digit\n",
    "    \n",
    "    def as_instance(self, x, skip_inference : bool = False, *args, **kwargs):\n",
    "        if not skip_inference:\n",
    "            x = self.forward(x)\n",
    "        data = self._co_property.intepret(x)\n",
    "        data.update(**kwargs)\n",
    "        return self._as_object(**data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0b9dee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = DigitProblem(_id=\"digit_problem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb9ebbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitLearner(LightningLearner):\n",
    "    def __init__(self, problem, *args, **kwargs):\n",
    "        super().__init__(problem, *args, **kwargs)\n",
    "        self.loss_infor = (\"digit\", nn.CrossEntropyLoss())\n",
    "    \n",
    "    def _aggerate_loss(self, y_hat, y, *args, **kwargs):\n",
    "        tmp = {  }\n",
    "        tmp[\"digit\"] = self.loss_infor[\"digit\"](y_hat[0], y)\n",
    "        tmp[\"total_loss\"] = tmp[\"digit\"]\n",
    "        return tmp\n",
    "    \n",
    "    def training_step(self, batch, batch_idx, *args, **kwargs):\n",
    "        return super().training_step(batch, batch_idx, on_step=True, *args, **kwargs)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx, *args, **kwargs):\n",
    "        return super().validation_step(batch, batch_idx, on_step=True ,*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e5aeb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T·ªïng s·ªë tham s·ªë:  332444\n",
      "T·ªïng s·ªë tham s·ªë hu·∫•n luy·ªán:  332444\n"
     ]
    }
   ],
   "source": [
    "learner = DigitLearner(solver)\n",
    "for _id in learner.learnable.keys():\n",
    "    learner.learnable = (_id, True)\n",
    "learner.compile(optim.SGD, device=\"cuda:0\", lr=0.01)\n",
    "print(\"T·ªïng s·ªë tham s·ªë: \", learner.total_parameters(solver))\n",
    "print(\"T·ªïng s·ªë tham s·ªë hu·∫•n luy·ªán: \", learner.total_learnable_parameters(solver))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fced078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=32, shuffle=True, num_workers=7, persistent_workers=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=32, shuffle=True, num_workers=7, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc850e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name     | Type         | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | _problem | DigitProblem | 332 K  | train\n",
      "--------------------------------------------------\n",
      "332 K     Trainable params\n",
      "0         Non-trainable params\n",
      "332 K     Total params\n",
      "1.330     Total estimated model params size (MB)\n",
      "46        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\simulations\\implementations\\env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1875/1875 [05:17<00:00,  5.91it/s, v_num=5, train_loss_step=2.300, val_loss_step=2.300, val_loss_epoch=2.300, train_loss_epoch=2.300]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1875/1875 [05:17<00:00,  5.91it/s, v_num=5, train_loss_step=2.300, val_loss_step=2.300, val_loss_epoch=2.300, train_loss_epoch=2.300]\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(accelerator=\"auto\", min_epochs=3, max_epochs=10)\n",
    "trainer.fit(learner, train_loader, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bde8d5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%self.1 : __torch__.DigitProblem,\n",
      "      %x.1 : Float(1, 1, 28, 28, strides=[784, 784, 28, 1], requires_grad=0, device=cpu)):\n",
      "  %_co_property : __torch__.fgi.layer.CoPropertyLayer.CoPropertyLayer = prim::GetAttr[name=\"_co_property\"](%self.1)\n",
      "  %_property : __torch__.fgi.layer.PropertyLayer.PropertyLayer = prim::GetAttr[name=\"_property\"](%self.1)\n",
      "  %_co_represent : __torch__.fgi.layer.CoRepresentLayer.CoRepresentLayer = prim::GetAttr[name=\"_co_represent\"](%self.1)\n",
      "  %_represent : __torch__.fgi.layer.RepresentLayer.RepresentLayer = prim::GetAttr[name=\"_represent\"](%self.1)\n",
      "  %1067 : int = prim::Constant[value=-1](), scope: __module._represent/__module._represent._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:55:0\n",
      "  %1068 : int = prim::Constant[value=4](), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._patch_embedding/__module._represent._units.0._patch_embedding.0 # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549:0\n",
      "  %1069 : int = prim::Constant[value=1](), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._patch_embedding/__module._represent._units.0._patch_embedding.0 # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549:0\n",
      "  %1070 : bool = prim::Constant[value=1](), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._patch_embedding/__module._represent._units.0._patch_embedding.0 # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549:0\n",
      "  %1071 : Double(requires_grad=0, device=cpu) = prim::Constant[value={0.0883883}](), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6362:0\n",
      "  %1072 : int = prim::Constant[value=-2](), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5619:0\n",
      "  %1073 : int = prim::Constant[value=3](), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\_tensor.py:1432:0\n",
      "  %1074 : str = prim::Constant[value=\"trunc\"](), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6207:0\n",
      "  %1075 : Long(requires_grad=0, device=cpu) = prim::Constant[value={1}](), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6207:0\n",
      "  %1076 : int = prim::Constant[value=9223372036854775807](), scope: __module._represent/__module._represent._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:76:0\n",
      "  %1077 : bool = prim::Constant[value=0](), scope: __module._represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:27:0\n",
      "  %1078 : Device = prim::Constant[value=\"cpu\"](), scope: __module._represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:27:0\n",
      "  %1079 : NoneType = prim::Constant(), scope: __module._represent\n",
      "  %1080 : int = prim::Constant[value=6](), scope: __module._represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:27:0\n",
      "  %1081 : int = prim::Constant[value=128](), scope: __module._represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:27:0\n",
      "  %1082 : int = prim::Constant[value=2](), scope: __module._represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:27:0\n",
      "  %1083 : int = prim::Constant[value=0](), scope: __module._represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:20:0\n",
      "  %_units.3 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"_units\"](%_represent)\n",
      "  %_1.5 : __torch__.fgi.problem.vision.represent.___torch_mangle_5.ImageRepresent = prim::GetAttr[name=\"1\"](%_units.3)\n",
      "  %_units.1 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"_units\"](%_represent)\n",
      "  %_0.3 : __torch__.fgi.problem.vision.represent.ImageRepresent = prim::GetAttr[name=\"0\"](%_units.1)\n",
      "  %1088 : int = aten::size(%x.1, %1083), scope: __module._represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:20:0\n",
      "  %1089 : int[] = prim::ListConstruct(%1082, %1088, %1081), scope: __module._represent\n",
      "  %tempo.1 : Float(2, 1, 128, strides=[128, 128, 1], requires_grad=0, device=cpu) = aten::empty(%1089, %1080, %1079, %1078, %1077, %1079), scope: __module._represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:27:0\n",
      "  %_attn.1 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"_attn\"](%_0.3)\n",
      "  %_pos_embed.1 : Tensor = prim::GetAttr[name=\"_pos_embed\"](%_0.3)\n",
      "  %_patch_embedding.1 : __torch__.torch.nn.modules.container.Sequential = prim::GetAttr[name=\"_patch_embedding\"](%_0.3)\n",
      "  %_cls_token.1 : Tensor = prim::GetAttr[name=\"_cls_token\"](%_0.3)\n",
      "  %1095 : int = aten::size(%x.1, %1083), scope: __module._represent/__module._represent._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:55:0\n",
      "  %1096 : int[] = prim::ListConstruct(%1095, %1067, %1067), scope: __module._represent/__module._represent._units.0\n",
      "  %cls_token.1 : Float(1, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::expand(%_cls_token.1, %1096, %1077), scope: __module._represent/__module._represent._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:55:0\n",
      "  %_0.1 : __torch__.torch.nn.modules.conv.Conv2d = prim::GetAttr[name=\"0\"](%_patch_embedding.1)\n",
      "  %bias.5 : Tensor = prim::GetAttr[name=\"bias\"](%_0.1)\n",
      "  %weight.5 : Tensor = prim::GetAttr[name=\"weight\"](%_0.1)\n",
      "  %1101 : int[] = prim::ListConstruct(%1068, %1068), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._patch_embedding/__module._represent._units.0._patch_embedding.0\n",
      "  %1102 : int[] = prim::ListConstruct(%1083, %1083), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._patch_embedding/__module._represent._units.0._patch_embedding.0\n",
      "  %1103 : int[] = prim::ListConstruct(%1069, %1069), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._patch_embedding/__module._represent._units.0._patch_embedding.0\n",
      "  %1104 : int[] = prim::ListConstruct(%1083, %1083), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._patch_embedding/__module._represent._units.0._patch_embedding.0\n",
      "  %input.1 : Float(1, 128, 7, 7, strides=[6272, 49, 7, 1], requires_grad=0, device=cpu) = aten::_convolution(%x.1, %weight.5, %bias.5, %1101, %1102, %1103, %1077, %1104, %1069, %1077, %1077, %1070, %1070), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._patch_embedding/__module._represent._units.0._patch_embedding.0 # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549:0\n",
      "  %x.3 : Float(1, 128, 49, strides=[6272, 49, 1], requires_grad=1, device=cpu) = aten::flatten(%input.1, %1082, %1067), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._patch_embedding/__module._represent._units.0._patch_embedding.1 # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\flatten.py:53:0\n",
      "  %1107 : int[] = prim::ListConstruct(%1083, %1082, %1069), scope: __module._represent/__module._represent._units.0\n",
      "  %x.5 : Float(1, 49, 128, strides=[6272, 1, 49], requires_grad=1, device=cpu) = aten::permute(%x.3, %1107), scope: __module._represent/__module._represent._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:59:0\n",
      "  %1109 : Tensor[] = prim::ListConstruct(%cls_token.1, %x.5), scope: __module._represent/__module._represent._units.0\n",
      "  %x.7 : Float(1, 50, 128, strides=[6400, 128, 1], requires_grad=1, device=cpu) = aten::cat(%1109, %1069), scope: __module._represent/__module._represent._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:60:0\n",
      "  %query.1 : Float(1, 50, 128, strides=[6400, 128, 1], requires_grad=1, device=cpu) = aten::add(%_pos_embed.1, %x.7, %1069), scope: __module._represent/__module._represent._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:62:0\n",
      "  %out_proj.3 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%_attn.1)\n",
      "  %bias.7 : Tensor = prim::GetAttr[name=\"bias\"](%out_proj.3)\n",
      "  %out_proj.1 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%_attn.1)\n",
      "  %weight.7 : Tensor = prim::GetAttr[name=\"weight\"](%out_proj.1)\n",
      "  %in_proj_bias.1 : Tensor = prim::GetAttr[name=\"in_proj_bias\"](%_attn.1)\n",
      "  %in_proj_weight.1 : Tensor = prim::GetAttr[name=\"in_proj_weight\"](%_attn.1)\n",
      "  %query.3 : Float(50, 1, 128, strides=[128, 6400, 1], requires_grad=1, device=cpu) = aten::transpose(%query.1, %1069, %1083), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1339:0\n",
      "  %1119 : int = aten::size(%query.3, %1083), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n",
      "  %tgt_len.1 : Long(device=cpu) = prim::NumToTensor(%1119), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %1121 : int = aten::size(%query.3, %1069), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n",
      "  %bsz.1 : Long(device=cpu) = prim::NumToTensor(%1121), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %1123 : int = aten::size(%query.3, %1082), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n",
      "  %embed_dim.1 : Long(device=cpu) = prim::NumToTensor(%1123), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %head_dim.1 : Long(requires_grad=0, device=cpu) = aten::div(%embed_dim.1, %1075, %1074), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6207:0\n",
      "  %1126 : int = aten::Int(%head_dim.1), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %1127 : int = aten::Int(%head_dim.1), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %1128 : int = aten::Int(%head_dim.1), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %1129 : int = aten::size(%query.3, %1067), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5610:0\n",
      "  %1130 : Float(50, 1, 384, strides=[384, 384, 1], requires_grad=1, device=cpu) = aten::linear(%query.3, %in_proj_weight.1, %in_proj_bias.1), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5614:0\n",
      "  %1131 : int[] = prim::ListConstruct(%1073, %1129), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %1132 : Float(50, 1, 3, 128, strides=[384, 384, 128, 1], requires_grad=1, device=cpu) = aten::unflatten(%1130, %1067, %1131), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\_tensor.py:1432:0\n",
      "  %1133 : Float(1, 50, 1, 3, 128, strides=[19200, 384, 384, 128, 1], requires_grad=1, device=cpu) = aten::unsqueeze(%1132, %1083), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5618:0\n",
      "  %1134 : Float(3, 50, 1, 1, 128, strides=[128, 384, 384, 19200, 1], requires_grad=1, device=cpu) = aten::transpose(%1133, %1083, %1072), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5619:0\n",
      "  %1135 : Float(3, 50, 1, 128, strides=[128, 384, 384, 1], requires_grad=1, device=cpu) = aten::squeeze(%1134, %1072), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5620:0\n",
      "  %proj.1 : Float(3, 50, 1, 128, strides=[6400, 128, 128, 1], requires_grad=1, device=cpu) = aten::contiguous(%1135, %1083), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5621:0\n",
      "  %q.1 : Float(50, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::select(%proj.1, %1083, %1083), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n",
      "  %k.1 : Float(50, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::select(%proj.1, %1083, %1069), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n",
      "  %v.1 : Float(50, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::select(%proj.1, %1083, %1082), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n",
      "  %1140 : Long(requires_grad=0, device=cpu) = aten::mul(%bsz.1, %1075), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n",
      "  %1141 : int = aten::Int(%1140), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %1142 : int[] = prim::ListConstruct(%1119, %1141, %1128), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %1143 : Float(50, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::view(%q.1, %1142), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n",
      "  %q.3 : Float(1, 50, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::transpose(%1143, %1083, %1069), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n",
      "  %1145 : int = aten::size(%k.1, %1083), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n",
      "  %1146 : Long(requires_grad=0, device=cpu) = aten::mul(%bsz.1, %1075), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n",
      "  %1147 : int = aten::Int(%1146), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %1148 : int[] = prim::ListConstruct(%1145, %1147, %1127), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %1149 : Float(50, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::view(%k.1, %1148), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n",
      "  %k.3 : Float(1, 50, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::transpose(%1149, %1083, %1069), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n",
      "  %1151 : int = aten::size(%v.1, %1083), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n",
      "  %1152 : Long(requires_grad=0, device=cpu) = aten::mul(%bsz.1, %1075), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n",
      "  %1153 : int = aten::Int(%1152), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %1154 : int[] = prim::ListConstruct(%1151, %1153, %1126), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %1155 : Float(50, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::view(%v.1, %1154), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n",
      "  %v.3 : Float(1, 50, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::transpose(%1155, %1083, %1069), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n",
      "  %q_scaled.1 : Float(1, 50, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::mul(%q.3, %1071), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6362:0\n",
      "  %1158 : Float(1, 128, 50, strides=[128, 1, 128], requires_grad=1, device=cpu) = aten::transpose(%k.3, %1072, %1067), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6373:0\n",
      "  %input.3 : Float(1, 50, 50, strides=[2500, 50, 1], requires_grad=1, device=cpu) = aten::bmm(%q_scaled.1, %1158), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6373:0\n",
      "  %attn_output_weights.1 : Float(1, 50, 50, strides=[2500, 50, 1], requires_grad=1, device=cpu) = aten::softmax(%input.3, %1067, %1079), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:2140:0\n",
      "  %attn_output.1 : Float(1, 50, 128, strides=[6400, 128, 1], requires_grad=1, device=cpu) = aten::bmm(%attn_output_weights.1, %v.3), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6378:0\n",
      "  %1162 : Float(50, 1, 128, strides=[128, 6400, 1], requires_grad=1, device=cpu) = aten::transpose(%attn_output.1, %1083, %1069), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6381:0\n",
      "  %1163 : Float(50, 1, 128, strides=[128, 6400, 1], requires_grad=1, device=cpu) = aten::contiguous(%1162, %1083), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6381:0\n",
      "  %1164 : Long(requires_grad=0, device=cpu) = aten::mul(%tgt_len.1, %bsz.1), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6381:0\n",
      "  %1165 : int = aten::Int(%1164), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %1166 : int[] = prim::ListConstruct(%1165, %1123), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %attn_output.3 : Float(50, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::view(%1163, %1166), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6381:0\n",
      "  %attn_output.5 : Float(50, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::linear(%attn_output.3, %weight.7, %bias.7), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6383:0\n",
      "  %1169 : int = aten::size(%attn_output.5, %1069), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6384:0\n",
      "  %1170 : int[] = prim::ListConstruct(%1119, %1121, %1169), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %attn_output.7 : Float(50, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::view(%attn_output.5, %1170), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6384:0\n",
      "  %x.9 : Float(1, 50, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::transpose(%attn_output.7, %1069, %1083), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1395:0\n",
      "  %1173 : Float(1, 50, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::slice(%x.9, %1083, %1083, %1076, %1069), scope: __module._represent/__module._represent._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:76:0\n",
      "  %1174 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::select(%1173, %1069, %1083), scope: __module._represent/__module._represent._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:76:0\n",
      "  %1175 : Float(1, 128, strides=[128, 1], requires_grad=0, device=cpu) = aten::select(%tempo.1, %1083, %1083), scope: __module._represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %1176 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::copy_(%1175, %1174, %1077), scope: __module._represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %_attn : __torch__.torch.nn.modules.activation.___torch_mangle_4.MultiheadAttention = prim::GetAttr[name=\"_attn\"](%_1.5)\n",
      "  %_pos_embed : Tensor = prim::GetAttr[name=\"_pos_embed\"](%_1.5)\n",
      "  %_patch_embedding : __torch__.torch.nn.modules.container.___torch_mangle_2.Sequential = prim::GetAttr[name=\"_patch_embedding\"](%_1.5)\n",
      "  %_cls_token : Tensor = prim::GetAttr[name=\"_cls_token\"](%_1.5)\n",
      "  %1181 : int = aten::size(%x.1, %1083), scope: __module._represent/__module._represent._units.1 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:55:0\n",
      "  %1182 : int[] = prim::ListConstruct(%1181, %1067, %1067), scope: __module._represent/__module._represent._units.1\n",
      "  %cls_token : Float(1, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::expand(%_cls_token, %1182, %1077), scope: __module._represent/__module._represent._units.1 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:55:0\n",
      "  %_0.5 : __torch__.torch.nn.modules.conv.___torch_mangle_0.Conv2d = prim::GetAttr[name=\"0\"](%_patch_embedding)\n",
      "  %bias.9 : Tensor = prim::GetAttr[name=\"bias\"](%_0.5)\n",
      "  %weight.9 : Tensor = prim::GetAttr[name=\"weight\"](%_0.5)\n",
      "  %1187 : int[] = prim::ListConstruct(%1068, %1068), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._patch_embedding/__module._represent._units.1._patch_embedding.0\n",
      "  %1188 : int[] = prim::ListConstruct(%1083, %1083), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._patch_embedding/__module._represent._units.1._patch_embedding.0\n",
      "  %1189 : int[] = prim::ListConstruct(%1069, %1069), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._patch_embedding/__module._represent._units.1._patch_embedding.0\n",
      "  %1190 : int[] = prim::ListConstruct(%1083, %1083), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._patch_embedding/__module._represent._units.1._patch_embedding.0\n",
      "  %input.5 : Float(1, 128, 7, 7, strides=[6272, 49, 7, 1], requires_grad=0, device=cpu) = aten::_convolution(%x.1, %weight.9, %bias.9, %1187, %1188, %1189, %1077, %1190, %1069, %1077, %1077, %1070, %1070), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._patch_embedding/__module._represent._units.1._patch_embedding.0 # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549:0\n",
      "  %x.11 : Float(1, 128, 49, strides=[6272, 49, 1], requires_grad=1, device=cpu) = aten::flatten(%input.5, %1082, %1067), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._patch_embedding/__module._represent._units.1._patch_embedding.1 # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\flatten.py:53:0\n",
      "  %1193 : int[] = prim::ListConstruct(%1083, %1082, %1069), scope: __module._represent/__module._represent._units.1\n",
      "  %x.13 : Float(1, 49, 128, strides=[6272, 1, 49], requires_grad=1, device=cpu) = aten::permute(%x.11, %1193), scope: __module._represent/__module._represent._units.1 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:59:0\n",
      "  %1195 : Tensor[] = prim::ListConstruct(%cls_token, %x.13), scope: __module._represent/__module._represent._units.1\n",
      "  %x.15 : Float(1, 50, 128, strides=[6400, 128, 1], requires_grad=1, device=cpu) = aten::cat(%1195, %1069), scope: __module._represent/__module._represent._units.1 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:60:0\n",
      "  %query.5 : Float(1, 50, 128, strides=[6400, 128, 1], requires_grad=1, device=cpu) = aten::add(%_pos_embed, %x.15, %1069), scope: __module._represent/__module._represent._units.1 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:62:0\n",
      "  %out_proj : __torch__.torch.nn.modules.linear.___torch_mangle_3.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%_attn)\n",
      "  %bias.11 : Tensor = prim::GetAttr[name=\"bias\"](%out_proj)\n",
      "  %out_proj.5 : __torch__.torch.nn.modules.linear.___torch_mangle_3.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%_attn)\n",
      "  %weight.11 : Tensor = prim::GetAttr[name=\"weight\"](%out_proj.5)\n",
      "  %in_proj_bias : Tensor = prim::GetAttr[name=\"in_proj_bias\"](%_attn)\n",
      "  %in_proj_weight : Tensor = prim::GetAttr[name=\"in_proj_weight\"](%_attn)\n",
      "  %query : Float(50, 1, 128, strides=[128, 6400, 1], requires_grad=1, device=cpu) = aten::transpose(%query.5, %1069, %1083), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1339:0\n",
      "  %1205 : int = aten::size(%query, %1083), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n",
      "  %tgt_len : Long(device=cpu) = prim::NumToTensor(%1205), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %1207 : int = aten::size(%query, %1069), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n",
      "  %bsz : Long(device=cpu) = prim::NumToTensor(%1207), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %1209 : int = aten::size(%query, %1082), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n",
      "  %embed_dim : Long(device=cpu) = prim::NumToTensor(%1209), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %head_dim : Long(requires_grad=0, device=cpu) = aten::div(%embed_dim, %1075, %1074), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6207:0\n",
      "  %1212 : int = aten::Int(%head_dim), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %1213 : int = aten::Int(%head_dim), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %1214 : int = aten::Int(%head_dim), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %1215 : int = aten::size(%query, %1067), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5610:0\n",
      "  %1216 : Float(50, 1, 384, strides=[384, 384, 1], requires_grad=1, device=cpu) = aten::linear(%query, %in_proj_weight, %in_proj_bias), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5614:0\n",
      "  %1217 : int[] = prim::ListConstruct(%1073, %1215), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %1218 : Float(50, 1, 3, 128, strides=[384, 384, 128, 1], requires_grad=1, device=cpu) = aten::unflatten(%1216, %1067, %1217), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\_tensor.py:1432:0\n",
      "  %1219 : Float(1, 50, 1, 3, 128, strides=[19200, 384, 384, 128, 1], requires_grad=1, device=cpu) = aten::unsqueeze(%1218, %1083), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5618:0\n",
      "  %1220 : Float(3, 50, 1, 1, 128, strides=[128, 384, 384, 19200, 1], requires_grad=1, device=cpu) = aten::transpose(%1219, %1083, %1072), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5619:0\n",
      "  %1221 : Float(3, 50, 1, 128, strides=[128, 384, 384, 1], requires_grad=1, device=cpu) = aten::squeeze(%1220, %1072), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5620:0\n",
      "  %proj : Float(3, 50, 1, 128, strides=[6400, 128, 128, 1], requires_grad=1, device=cpu) = aten::contiguous(%1221, %1083), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5621:0\n",
      "  %q.5 : Float(50, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::select(%proj, %1083, %1083), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n",
      "  %k.5 : Float(50, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::select(%proj, %1083, %1069), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n",
      "  %v.5 : Float(50, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::select(%proj, %1083, %1082), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n",
      "  %1226 : Long(requires_grad=0, device=cpu) = aten::mul(%bsz, %1075), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n",
      "  %1227 : int = aten::Int(%1226), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %1228 : int[] = prim::ListConstruct(%1205, %1227, %1214), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %1229 : Float(50, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::view(%q.5, %1228), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n",
      "  %q : Float(1, 50, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::transpose(%1229, %1083, %1069), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n",
      "  %1231 : int = aten::size(%k.5, %1083), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n",
      "  %1232 : Long(requires_grad=0, device=cpu) = aten::mul(%bsz, %1075), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n",
      "  %1233 : int = aten::Int(%1232), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %1234 : int[] = prim::ListConstruct(%1231, %1233, %1213), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %1235 : Float(50, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::view(%k.5, %1234), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n",
      "  %k : Float(1, 50, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::transpose(%1235, %1083, %1069), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n",
      "  %1237 : int = aten::size(%v.5, %1083), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n",
      "  %1238 : Long(requires_grad=0, device=cpu) = aten::mul(%bsz, %1075), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n",
      "  %1239 : int = aten::Int(%1238), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %1240 : int[] = prim::ListConstruct(%1237, %1239, %1212), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %1241 : Float(50, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::view(%v.5, %1240), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n",
      "  %v : Float(1, 50, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::transpose(%1241, %1083, %1069), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n",
      "  %q_scaled : Float(1, 50, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::mul(%q, %1071), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6362:0\n",
      "  %1244 : Float(1, 128, 50, strides=[128, 1, 128], requires_grad=1, device=cpu) = aten::transpose(%k, %1072, %1067), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6373:0\n",
      "  %input.7 : Float(1, 50, 50, strides=[2500, 50, 1], requires_grad=1, device=cpu) = aten::bmm(%q_scaled, %1244), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6373:0\n",
      "  %attn_output_weights.5 : Float(1, 50, 50, strides=[2500, 50, 1], requires_grad=1, device=cpu) = aten::softmax(%input.7, %1067, %1079), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:2140:0\n",
      "  %attn_output.9 : Float(1, 50, 128, strides=[6400, 128, 1], requires_grad=1, device=cpu) = aten::bmm(%attn_output_weights.5, %v), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6378:0\n",
      "  %1248 : Float(50, 1, 128, strides=[128, 6400, 1], requires_grad=1, device=cpu) = aten::transpose(%attn_output.9, %1083, %1069), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6381:0\n",
      "  %1249 : Float(50, 1, 128, strides=[128, 6400, 1], requires_grad=1, device=cpu) = aten::contiguous(%1248, %1083), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6381:0\n",
      "  %1250 : Long(requires_grad=0, device=cpu) = aten::mul(%tgt_len, %bsz), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6381:0\n",
      "  %1251 : int = aten::Int(%1250), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %1252 : int[] = prim::ListConstruct(%1251, %1209), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %attn_output.11 : Float(50, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::view(%1249, %1252), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6381:0\n",
      "  %attn_output.13 : Float(50, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::linear(%attn_output.11, %weight.11, %bias.11), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6383:0\n",
      "  %1255 : int = aten::size(%attn_output.13, %1069), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6384:0\n",
      "  %1256 : int[] = prim::ListConstruct(%1205, %1207, %1255), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %attn_output : Float(50, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::view(%attn_output.13, %1256), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6384:0\n",
      "  %x.17 : Float(1, 50, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::transpose(%attn_output, %1069, %1083), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1395:0\n",
      "  %1259 : Float(1, 50, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::slice(%x.17, %1083, %1083, %1076, %1069), scope: __module._represent/__module._represent._units.1 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:76:0\n",
      "  %1260 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::select(%1259, %1069, %1083), scope: __module._represent/__module._represent._units.1 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:76:0\n",
      "  %1261 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::select(%tempo.1, %1083, %1069), scope: __module._represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %1262 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::copy_(%1261, %1260, %1077), scope: __module._represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %x.19 : Float(1, 2, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::transpose(%tempo.1, %1083, %1069), scope: __module._represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:32:0\n",
      "  %1264 : int = prim::Constant[value=1](), scope: __module._co_represent/__module._co_represent._units.0/__module._co_represent._units.0._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:36:0\n",
      "  %1265 : bool = prim::Constant[value=0](), scope: __module._co_represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:27:0\n",
      "  %1266 : Device = prim::Constant[value=\"cpu\"](), scope: __module._co_represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:27:0\n",
      "  %1267 : NoneType = prim::Constant(), scope: __module._co_represent\n",
      "  %1268 : int = prim::Constant[value=6](), scope: __module._co_represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:27:0\n",
      "  %1269 : int = prim::Constant[value=2](), scope: __module._co_represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:22:0\n",
      "  %1270 : int = prim::Constant[value=0](), scope: __module._co_represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:20:0\n",
      "  %_units.7 : __torch__.torch.nn.modules.container.___torch_mangle_10.ModuleList = prim::GetAttr[name=\"_units\"](%_co_represent)\n",
      "  %_1.7 : __torch__.fgi.units.co_represent.___torch_mangle_9.CoRepresentUnit = prim::GetAttr[name=\"1\"](%_units.7)\n",
      "  %_units.5 : __torch__.torch.nn.modules.container.___torch_mangle_10.ModuleList = prim::GetAttr[name=\"_units\"](%_co_represent)\n",
      "  %_0.7 : __torch__.fgi.units.co_represent.CoRepresentUnit = prim::GetAttr[name=\"0\"](%_units.5)\n",
      "  %1275 : int = aten::size(%x.19, %1270), scope: __module._co_represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:20:0\n",
      "  %1276 : int = aten::size(%x.19, %1269), scope: __module._co_represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:22:0\n",
      "  %1277 : int[] = prim::ListConstruct(%1269, %1275, %1276), scope: __module._co_represent\n",
      "  %tempo.3 : Float(2, 1, 128, strides=[128, 128, 1], requires_grad=0, device=cpu) = aten::empty(%1277, %1268, %1267, %1266, %1265, %1267), scope: __module._co_represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:27:0\n",
      "  %_combine.1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"_combine\"](%_0.7)\n",
      "  %_memory.1 : __torch__.fgi.units.memory.SoftMemoryUnit = prim::GetAttr[name=\"_memory\"](%_0.7)\n",
      "  %_weighted.1 : Tensor = prim::GetAttr[name=\"_weighted\"](%_0.7)\n",
      "  %weighted.1 : Float(2, strides=[1], requires_grad=1, device=cpu) = aten::softmax(%_weighted.1, %1270, %1267), scope: __module._co_represent/__module._co_represent._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\co_represent.py:30:0\n",
      "  %x.21 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::matmul(%weighted.1, %x.19), scope: __module._co_represent/__module._co_represent._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\co_represent.py:31:0\n",
      "  %_patterns.1 : Tensor = prim::GetAttr[name=\"_patterns\"](%_memory.1)\n",
      "  %_beta.1 : Tensor = prim::GetAttr[name=\"_beta\"](%_memory.1)\n",
      "  %1286 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::mul(%_beta.1, %x.21), scope: __module._co_represent/__module._co_represent._units.0/__module._co_represent._units.0._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %1287 : Float(128, 128, strides=[1, 128], requires_grad=1, device=cpu) = aten::numpy_T(%_patterns.1), scope: __module._co_represent/__module._co_represent._units.0/__module._co_represent._units.0._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %weighted.3 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::matmul(%1286, %1287), scope: __module._co_represent/__module._co_represent._units.0/__module._co_represent._units.0._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %weighted.5 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::softmax(%weighted.3, %1264, %1267), scope: __module._co_represent/__module._co_represent._units.0/__module._co_represent._units.0._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:36:0\n",
      "  %p.1 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::matmul(%weighted.5, %_patterns.1), scope: __module._co_represent/__module._co_represent._units.0/__module._co_represent._units.0._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:37:0\n",
      "  %1291 : Tensor[] = prim::ListConstruct(%x.21, %p.1), scope: __module._co_represent/__module._co_represent._units.0\n",
      "  %input.9 : Float(1, 256, strides=[256, 1], requires_grad=1, device=cpu) = aten::cat(%1291, %1264), scope: __module._co_represent/__module._co_represent._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\co_represent.py:37:0\n",
      "  %bias.13 : Tensor = prim::GetAttr[name=\"bias\"](%_combine.1)\n",
      "  %weight.13 : Tensor = prim::GetAttr[name=\"weight\"](%_combine.1)\n",
      "  %input.11 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::linear(%input.9, %weight.13, %bias.13), scope: __module._co_represent/__module._co_represent._units.0/__module._co_represent._units.0._combine # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n",
      "  %1296 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::relu(%input.11), scope: __module._co_represent/__module._co_represent._units.0/__module._co_represent._units.0._activate # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:1704:0\n",
      "  %1297 : Float(1, 128, strides=[128, 1], requires_grad=0, device=cpu) = aten::select(%tempo.3, %1270, %1270), scope: __module._co_represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %1298 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::copy_(%1297, %1296, %1265), scope: __module._co_represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %_combine : __torch__.torch.nn.modules.linear.___torch_mangle_7.Linear = prim::GetAttr[name=\"_combine\"](%_1.7)\n",
      "  %_memory.3 : __torch__.fgi.units.memory.___torch_mangle_6.SoftMemoryUnit = prim::GetAttr[name=\"_memory\"](%_1.7)\n",
      "  %_weighted.3 : Tensor = prim::GetAttr[name=\"_weighted\"](%_1.7)\n",
      "  %weighted.7 : Float(2, strides=[1], requires_grad=1, device=cpu) = aten::softmax(%_weighted.3, %1270, %1267), scope: __module._co_represent/__module._co_represent._units.1 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\co_represent.py:30:0\n",
      "  %x.23 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::matmul(%weighted.7, %x.19), scope: __module._co_represent/__module._co_represent._units.1 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\co_represent.py:31:0\n",
      "  %_patterns.3 : Tensor = prim::GetAttr[name=\"_patterns\"](%_memory.3)\n",
      "  %_beta.3 : Tensor = prim::GetAttr[name=\"_beta\"](%_memory.3)\n",
      "  %1306 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::mul(%_beta.3, %x.23), scope: __module._co_represent/__module._co_represent._units.1/__module._co_represent._units.1._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %1307 : Float(128, 128, strides=[1, 128], requires_grad=1, device=cpu) = aten::numpy_T(%_patterns.3), scope: __module._co_represent/__module._co_represent._units.1/__module._co_represent._units.1._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %weighted.9 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::matmul(%1306, %1307), scope: __module._co_represent/__module._co_represent._units.1/__module._co_represent._units.1._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %weighted.11 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::softmax(%weighted.9, %1264, %1267), scope: __module._co_represent/__module._co_represent._units.1/__module._co_represent._units.1._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:36:0\n",
      "  %p.3 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::matmul(%weighted.11, %_patterns.3), scope: __module._co_represent/__module._co_represent._units.1/__module._co_represent._units.1._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:37:0\n",
      "  %1311 : Tensor[] = prim::ListConstruct(%x.23, %p.3), scope: __module._co_represent/__module._co_represent._units.1\n",
      "  %input.13 : Float(1, 256, strides=[256, 1], requires_grad=1, device=cpu) = aten::cat(%1311, %1264), scope: __module._co_represent/__module._co_represent._units.1 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\co_represent.py:37:0\n",
      "  %bias.15 : Tensor = prim::GetAttr[name=\"bias\"](%_combine)\n",
      "  %weight.15 : Tensor = prim::GetAttr[name=\"weight\"](%_combine)\n",
      "  %input.15 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::linear(%input.13, %weight.15, %bias.15), scope: __module._co_represent/__module._co_represent._units.1/__module._co_represent._units.1._combine # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n",
      "  %1316 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::relu(%input.15), scope: __module._co_represent/__module._co_represent._units.1/__module._co_represent._units.1._activate # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:1704:0\n",
      "  %1317 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::select(%tempo.3, %1270, %1264), scope: __module._co_represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %1318 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::copy_(%1317, %1316, %1265), scope: __module._co_represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %x.25 : Float(1, 2, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::transpose(%tempo.3, %1270, %1264), scope: __module._co_represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:32:0\n",
      "  %x.27 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu), %1321 : Long(1, 128, strides=[128, 1], requires_grad=0, device=cpu) = aten::max(%x.25, %1264, %1265), scope: __module._co_represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\CoRepresentLayer.py:15:0\n",
      "  %1322 : int = prim::Constant[value=3](), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %1323 : int = prim::Constant[value=2](), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %1324 : bool = prim::Constant[value=0](), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:27:0\n",
      "  %1325 : Device = prim::Constant[value=\"cpu\"](), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:27:0\n",
      "  %1326 : NoneType = prim::Constant(), scope: __module._property\n",
      "  %1327 : int = prim::Constant[value=6](), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:27:0\n",
      "  %1328 : int = prim::Constant[value=4](), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:27:0\n",
      "  %1329 : int = prim::Constant[value=1](), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:22:0\n",
      "  %1330 : int = prim::Constant[value=0](), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:20:0\n",
      "  %_units.15 : __torch__.torch.nn.modules.container.___torch_mangle_22.ModuleList = prim::GetAttr[name=\"_units\"](%_property)\n",
      "  %_3 : __torch__.fgi.units.property.___torch_mangle_21.PropertyUnit = prim::GetAttr[name=\"3\"](%_units.15)\n",
      "  %_units.13 : __torch__.torch.nn.modules.container.___torch_mangle_22.ModuleList = prim::GetAttr[name=\"_units\"](%_property)\n",
      "  %_2 : __torch__.fgi.units.property.___torch_mangle_18.PropertyUnit = prim::GetAttr[name=\"2\"](%_units.13)\n",
      "  %_units.11 : __torch__.torch.nn.modules.container.___torch_mangle_22.ModuleList = prim::GetAttr[name=\"_units\"](%_property)\n",
      "  %_1.9 : __torch__.fgi.units.property.___torch_mangle_15.PropertyUnit = prim::GetAttr[name=\"1\"](%_units.11)\n",
      "  %_units.9 : __torch__.torch.nn.modules.container.___torch_mangle_22.ModuleList = prim::GetAttr[name=\"_units\"](%_property)\n",
      "  %_0.9 : __torch__.fgi.units.property.PropertyUnit = prim::GetAttr[name=\"0\"](%_units.9)\n",
      "  %1339 : int = aten::size(%x.27, %1330), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:20:0\n",
      "  %1340 : int = aten::size(%x.27, %1329), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:22:0\n",
      "  %1341 : int[] = prim::ListConstruct(%1328, %1339, %1340), scope: __module._property\n",
      "  %tempo : Float(4, 1, 128, strides=[128, 128, 1], requires_grad=0, device=cpu) = aten::empty(%1341, %1327, %1326, %1325, %1324, %1326), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:27:0\n",
      "  %_evaluation.1 : __torch__.torch.nn.modules.linear.___torch_mangle_12.Linear = prim::GetAttr[name=\"_evaluation\"](%_0.9)\n",
      "  %_memory.5 : __torch__.fgi.units.memory.___torch_mangle_11.SoftMemoryUnit = prim::GetAttr[name=\"_memory\"](%_0.9)\n",
      "  %_patterns.5 : Tensor = prim::GetAttr[name=\"_patterns\"](%_memory.5)\n",
      "  %_beta.5 : Tensor = prim::GetAttr[name=\"_beta\"](%_memory.5)\n",
      "  %1347 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::mul(%_beta.5, %x.27), scope: __module._property/__module._property._units.0/__module._property._units.0._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %1348 : Float(128, 128, strides=[1, 128], requires_grad=1, device=cpu) = aten::numpy_T(%_patterns.5), scope: __module._property/__module._property._units.0/__module._property._units.0._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %weighted.13 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::matmul(%1347, %1348), scope: __module._property/__module._property._units.0/__module._property._units.0._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %weighted.15 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::softmax(%weighted.13, %1329, %1326), scope: __module._property/__module._property._units.0/__module._property._units.0._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:36:0\n",
      "  %p.5 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::matmul(%weighted.15, %_patterns.5), scope: __module._property/__module._property._units.0/__module._property._units.0._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:37:0\n",
      "  %bias.17 : Tensor = prim::GetAttr[name=\"bias\"](%_evaluation.1)\n",
      "  %weight.17 : Tensor = prim::GetAttr[name=\"weight\"](%_evaluation.1)\n",
      "  %z.1 : Float(1, 1, strides=[1, 1], requires_grad=1, device=cpu) = aten::linear(%x.27, %weight.17, %bias.17), scope: __module._property/__module._property._units.0/__module._property._units.0._evaluation # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n",
      "  %z.3 : Float(1, 1, strides=[1, 1], requires_grad=1, device=cpu) = aten::sigmoid(%z.1), scope: __module._property/__module._property._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\property.py:22:0\n",
      "  %1356 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::mul(%p.5, %z.3), scope: __module._property/__module._property._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\property.py:25:0\n",
      "  %1357 : Float(1, 128, strides=[128, 1], requires_grad=0, device=cpu) = aten::select(%tempo, %1330, %1330), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %1358 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::copy_(%1357, %1356, %1324), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %_evaluation.3 : __torch__.torch.nn.modules.linear.___torch_mangle_14.Linear = prim::GetAttr[name=\"_evaluation\"](%_1.9)\n",
      "  %_memory.7 : __torch__.fgi.units.memory.___torch_mangle_13.SoftMemoryUnit = prim::GetAttr[name=\"_memory\"](%_1.9)\n",
      "  %_patterns.7 : Tensor = prim::GetAttr[name=\"_patterns\"](%_memory.7)\n",
      "  %_beta.7 : Tensor = prim::GetAttr[name=\"_beta\"](%_memory.7)\n",
      "  %1363 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::mul(%_beta.7, %x.27), scope: __module._property/__module._property._units.1/__module._property._units.1._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %1364 : Float(128, 128, strides=[1, 128], requires_grad=1, device=cpu) = aten::numpy_T(%_patterns.7), scope: __module._property/__module._property._units.1/__module._property._units.1._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %weighted.17 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::matmul(%1363, %1364), scope: __module._property/__module._property._units.1/__module._property._units.1._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %weighted.19 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::softmax(%weighted.17, %1329, %1326), scope: __module._property/__module._property._units.1/__module._property._units.1._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:36:0\n",
      "  %p.7 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::matmul(%weighted.19, %_patterns.7), scope: __module._property/__module._property._units.1/__module._property._units.1._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:37:0\n",
      "  %bias.19 : Tensor = prim::GetAttr[name=\"bias\"](%_evaluation.3)\n",
      "  %weight.19 : Tensor = prim::GetAttr[name=\"weight\"](%_evaluation.3)\n",
      "  %z.5 : Float(1, 1, strides=[1, 1], requires_grad=1, device=cpu) = aten::linear(%x.27, %weight.19, %bias.19), scope: __module._property/__module._property._units.1/__module._property._units.1._evaluation # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n",
      "  %z.7 : Float(1, 1, strides=[1, 1], requires_grad=1, device=cpu) = aten::sigmoid(%z.5), scope: __module._property/__module._property._units.1 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\property.py:22:0\n",
      "  %1372 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::mul(%p.7, %z.7), scope: __module._property/__module._property._units.1 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\property.py:25:0\n",
      "  %1373 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::select(%tempo, %1330, %1329), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %1374 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::copy_(%1373, %1372, %1324), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %_evaluation.5 : __torch__.torch.nn.modules.linear.___torch_mangle_17.Linear = prim::GetAttr[name=\"_evaluation\"](%_2)\n",
      "  %_memory.9 : __torch__.fgi.units.memory.___torch_mangle_16.SoftMemoryUnit = prim::GetAttr[name=\"_memory\"](%_2)\n",
      "  %_patterns.9 : Tensor = prim::GetAttr[name=\"_patterns\"](%_memory.9)\n",
      "  %_beta.9 : Tensor = prim::GetAttr[name=\"_beta\"](%_memory.9)\n",
      "  %1379 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::mul(%_beta.9, %x.27), scope: __module._property/__module._property._units.2/__module._property._units.2._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %1380 : Float(128, 128, strides=[1, 128], requires_grad=1, device=cpu) = aten::numpy_T(%_patterns.9), scope: __module._property/__module._property._units.2/__module._property._units.2._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %weighted.21 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::matmul(%1379, %1380), scope: __module._property/__module._property._units.2/__module._property._units.2._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %weighted.23 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::softmax(%weighted.21, %1329, %1326), scope: __module._property/__module._property._units.2/__module._property._units.2._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:36:0\n",
      "  %p.9 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::matmul(%weighted.23, %_patterns.9), scope: __module._property/__module._property._units.2/__module._property._units.2._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:37:0\n",
      "  %bias.21 : Tensor = prim::GetAttr[name=\"bias\"](%_evaluation.5)\n",
      "  %weight.21 : Tensor = prim::GetAttr[name=\"weight\"](%_evaluation.5)\n",
      "  %z.9 : Float(1, 1, strides=[1, 1], requires_grad=1, device=cpu) = aten::linear(%x.27, %weight.21, %bias.21), scope: __module._property/__module._property._units.2/__module._property._units.2._evaluation # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n",
      "  %z.11 : Float(1, 1, strides=[1, 1], requires_grad=1, device=cpu) = aten::sigmoid(%z.9), scope: __module._property/__module._property._units.2 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\property.py:22:0\n",
      "  %1388 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::mul(%p.9, %z.11), scope: __module._property/__module._property._units.2 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\property.py:25:0\n",
      "  %1389 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::select(%tempo, %1330, %1323), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %1390 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::copy_(%1389, %1388, %1324), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %_evaluation : __torch__.torch.nn.modules.linear.___torch_mangle_20.Linear = prim::GetAttr[name=\"_evaluation\"](%_3)\n",
      "  %_memory : __torch__.fgi.units.memory.___torch_mangle_19.SoftMemoryUnit = prim::GetAttr[name=\"_memory\"](%_3)\n",
      "  %_patterns : Tensor = prim::GetAttr[name=\"_patterns\"](%_memory)\n",
      "  %_beta : Tensor = prim::GetAttr[name=\"_beta\"](%_memory)\n",
      "  %1395 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::mul(%_beta, %x.27), scope: __module._property/__module._property._units.3/__module._property._units.3._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %1396 : Float(128, 128, strides=[1, 128], requires_grad=1, device=cpu) = aten::numpy_T(%_patterns), scope: __module._property/__module._property._units.3/__module._property._units.3._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %weighted.25 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::matmul(%1395, %1396), scope: __module._property/__module._property._units.3/__module._property._units.3._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %weighted.27 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::softmax(%weighted.25, %1329, %1326), scope: __module._property/__module._property._units.3/__module._property._units.3._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:36:0\n",
      "  %p : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::matmul(%weighted.27, %_patterns), scope: __module._property/__module._property._units.3/__module._property._units.3._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:37:0\n",
      "  %bias.23 : Tensor = prim::GetAttr[name=\"bias\"](%_evaluation)\n",
      "  %weight.23 : Tensor = prim::GetAttr[name=\"weight\"](%_evaluation)\n",
      "  %z.13 : Float(1, 1, strides=[1, 1], requires_grad=1, device=cpu) = aten::linear(%x.27, %weight.23, %bias.23), scope: __module._property/__module._property._units.3/__module._property._units.3._evaluation # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n",
      "  %z : Float(1, 1, strides=[1, 1], requires_grad=1, device=cpu) = aten::sigmoid(%z.13), scope: __module._property/__module._property._units.3 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\property.py:22:0\n",
      "  %1404 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::mul(%p, %z), scope: __module._property/__module._property._units.3 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\property.py:25:0\n",
      "  %1405 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::select(%tempo, %1330, %1322), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %1406 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::copy_(%1405, %1404, %1324), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %x.29 : Float(1, 4, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::transpose(%tempo, %1330, %1329), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:32:0\n",
      "  %1408 : int = prim::Constant[value=1](), scope: __module._co_property/__module._co_property._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\co_property.py:40:0\n",
      "  %1409 : int = prim::Constant[value=0](), scope: __module._co_property/__module._co_property._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\co_property.py:44:0\n",
      "  %1410 : NoneType = prim::Constant(), scope: __module._co_property/__module._co_property._units.0\n",
      "  %_units : __torch__.torch.nn.modules.container.___torch_mangle_27.ModuleList = prim::GetAttr[name=\"_units\"](%_co_property)\n",
      "  %_0 : __torch__.fgi.units.co_property.ChooseOptions = prim::GetAttr[name=\"0\"](%_units)\n",
      "  %_decides : __torch__.torch.nn.modules.linear.___torch_mangle_26.Linear = prim::GetAttr[name=\"_decides\"](%_0)\n",
      "  %_enhance : __torch__.torch.nn.modules.container.___torch_mangle_25.Sequential = prim::GetAttr[name=\"_enhance\"](%_0)\n",
      "  %_weighted : Tensor = prim::GetAttr[name=\"_weighted\"](%_0)\n",
      "  %_position : Tensor = prim::GetAttr[name=\"_position\"](%_0)\n",
      "  %x.31 : Float(1, 4, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::add(%x.29, %_position, %1408), scope: __module._co_property/__module._co_property._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\co_property.py:40:0\n",
      "  %weighted : Float(4, strides=[1], requires_grad=1, device=cpu) = aten::softmax(%_weighted, %1409, %1410), scope: __module._co_property/__module._co_property._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\co_property.py:44:0\n",
      "  %input.17 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::matmul(%weighted, %x.31), scope: __module._co_property/__module._co_property._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\co_property.py:45:0\n",
      "  %_1 : __torch__.torch.nn.modules.activation.___torch_mangle_24.ReLU = prim::GetAttr[name=\"1\"](%_enhance)\n",
      "  %_0.11 : __torch__.torch.nn.modules.linear.___torch_mangle_23.Linear = prim::GetAttr[name=\"0\"](%_enhance)\n",
      "  %bias.25 : Tensor = prim::GetAttr[name=\"bias\"](%_0.11)\n",
      "  %weight.25 : Tensor = prim::GetAttr[name=\"weight\"](%_0.11)\n",
      "  %input.19 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::linear(%input.17, %weight.25, %bias.25), scope: __module._co_property/__module._co_property._units.0/__module._co_property._units.0._enhance/__module._co_property._units.0._enhance.0 # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n",
      "  %input : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::relu(%input.19), scope: __module._co_property/__module._co_property._units.0/__module._co_property._units.0._enhance/__module._co_property._units.0._enhance.1 # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:1704:0\n",
      "  %bias : Tensor = prim::GetAttr[name=\"bias\"](%_decides)\n",
      "  %weight : Tensor = prim::GetAttr[name=\"weight\"](%_decides)\n",
      "  %x : Float(1, 10, strides=[10, 1], requires_grad=1, device=cpu) = aten::linear(%input, %weight, %bias), scope: __module._co_property/__module._co_property._units.0/__module._co_property._units.0._decides # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n",
      "  %1429 : Float(1, 10, strides=[10, 1], requires_grad=1, device=cpu) = aten::softmax(%x, %1408, %1410), scope: __module._co_property/__module._co_property._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\co_property.py:74:0\n",
      "  %884 : (Float(1, 10, strides=[10, 1], requires_grad=1, device=cpu)) = prim::TupleConstruct(%1429)\n",
      "  return (%884)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(\"runs/experiment\")\n",
    "writer.add_graph(solver, randn((1, 1, 28, 28)), verbose=True)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f29a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
