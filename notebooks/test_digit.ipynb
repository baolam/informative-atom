{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "385c52da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from fgi import *\n",
    "from torch import jit\n",
    "from torch import nn, randn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning import LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0f92faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.cuda import is_available\n",
    "is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a07e3e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Digit:\n",
    "    def __init__(self, digit ,*args, **kwargs):\n",
    "        self._digit = int(digit)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Digit({self._digit})\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67808e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitProblem(NonCodeProblem):\n",
    "    \"\"\"\n",
    "    Bộ phân loại chữ số\n",
    "    \"\"\"\n",
    "    def __init__(self, _id, *args, **kwargs):\n",
    "        super().__init__(_id, *args, **kwargs)\n",
    "        self._represent = RepresentLayer.from_units([\n",
    "            ImageRepresent(img_shape=(1, 28, 28), patch_size=4, num_heads=1, phi_dim=128),\n",
    "            ImageRepresent(img_shape=(1, 28, 28), patch_size=4, num_heads=1, phi_dim=128),\n",
    "            # EdgeRepresent(img_shape=(1, 28, 28), patch_size=4, num_heads=2, phi_dim=128)\n",
    "        ], output_dim=128)\n",
    "        self._co_represent = CoRepresentLayer.from_units(\n",
    "            [ CoRepresentUnit(2, phi_dim=128) for _ in range(2) ]\n",
    "        )\n",
    "        self._property = PropertyLayer.from_units(\n",
    "            [ PropertyUnit(phi_dim=128) for _ in range(4) ]\n",
    "        )\n",
    "        self._co_property = CoPropertyLayer.from_units(\n",
    "            [ ChooseOptions(4, options=[\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"], property_name=\"digit\", phi_dim=128) ]\n",
    "        )\n",
    "\n",
    "        self._update_additional_infor()\n",
    "    \n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        x = self._represent(x)\n",
    "        x = self._co_represent(x)\n",
    "        x = self._property(x)\n",
    "        x = self._co_property(x)\n",
    "        return x\n",
    "    \n",
    "    def recognize_unknown(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def _as_object(self):\n",
    "        return Digit\n",
    "    \n",
    "    def as_instance(self, x, skip_inference : bool = False, *args, **kwargs):\n",
    "        if not skip_inference:\n",
    "            x = self.forward(x)\n",
    "        data = self._co_property.intepret(x)\n",
    "        data.update(**kwargs)\n",
    "        return self._as_object(**data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0b9dee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = DigitProblem(_id=\"digit_problem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb9ebbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitLearner(LightningLearner):\n",
    "    def __init__(self, problem, *args, **kwargs):\n",
    "        super().__init__(problem, *args, **kwargs)\n",
    "        self.loss_infor = (\"digit\", nn.CrossEntropyLoss())\n",
    "    \n",
    "    def _aggerate_loss(self, y_hat, y, *args, **kwargs):\n",
    "        total_loss = 0.\n",
    "        tmp = {  }\n",
    "        for property in y_hat.keys():\n",
    "            l = self._loss_infor[property](y_hat[property], y)\n",
    "            total_loss += l\n",
    "            tmp[f\"loss_{property}\"] = l\n",
    "        tmp[\"total_loss\"] = total_loss\n",
    "        return tmp\n",
    "    \n",
    "    def training_step(self, batch, batch_idx, *args, **kwargs):\n",
    "        return super().training_step(batch, batch_idx, on_step=True, *args, **kwargs)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx, *args, **kwargs):\n",
    "        return super().validation_step(batch, batch_idx, on_step=True ,*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e5aeb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng số tham số:  334236\n",
      "Tổng số tham số huấn luyện:  334236\n"
     ]
    }
   ],
   "source": [
    "learner = DigitLearner(solver)\n",
    "for _id in learner.learnable.keys():\n",
    "    learner.learnable = (_id, True)\n",
    "learner.compile(optim.SGD, device=\"cuda:0\", lr=0.001)\n",
    "print(\"Tổng số tham số: \", learner.total_parameters(solver))\n",
    "print(\"Tổng số tham số huấn luyện: \", learner.total_learnable_parameters(solver))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fced078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=32, shuffle=True, num_workers=7, persistent_workers=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=32, shuffle=True, num_workers=7, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc850e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer = Trainer(accelerator=\"auto\", min_epochs=3, max_epochs=10)\n",
    "#trainer.fit(learner, train_loader, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bde8d5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%self.1 : __torch__.DigitProblem,\n",
      "      %x.1 : Float(1, 1, 28, 28, strides=[784, 784, 28, 1], requires_grad=0, device=cpu)):\n",
      "  %_co_property : __torch__.fgi.layer.CoPropertyLayer.CoPropertyLayer = prim::GetAttr[name=\"_co_property\"](%self.1)\n",
      "  %_property : __torch__.fgi.layer.PropertyLayer.PropertyLayer = prim::GetAttr[name=\"_property\"](%self.1)\n",
      "  %_co_represent : __torch__.fgi.layer.CoRepresentLayer.CoRepresentLayer = prim::GetAttr[name=\"_co_represent\"](%self.1)\n",
      "  %_represent : __torch__.fgi.layer.RepresentLayer.RepresentLayer = prim::GetAttr[name=\"_represent\"](%self.1)\n",
      "  %1245 : int = prim::Constant[value=-1](), scope: __module._represent/__module._represent._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:55:0\n",
      "  %1246 : int = prim::Constant[value=4](), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._patch_embedding/__module._represent._units.0._patch_embedding.0 # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549:0\n",
      "  %1247 : int = prim::Constant[value=1](), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._patch_embedding/__module._represent._units.0._patch_embedding.0 # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549:0\n",
      "  %1248 : bool = prim::Constant[value=1](), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._patch_embedding/__module._represent._units.0._patch_embedding.0 # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549:0\n",
      "  %1249 : float = prim::Constant[value=0.20000000000000001](), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._dropout # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n",
      "  %1250 : Double(requires_grad=0, device=cpu) = prim::Constant[value={0.0883883}](), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6362:0\n",
      "  %1251 : int = prim::Constant[value=-2](), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5619:0\n",
      "  %1252 : int = prim::Constant[value=3](), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\_tensor.py:1432:0\n",
      "  %1253 : str = prim::Constant[value=\"trunc\"](), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6207:0\n",
      "  %1254 : Long(requires_grad=0, device=cpu) = prim::Constant[value={1}](), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6207:0\n",
      "  %1255 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._norm # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n",
      "  %1256 : int = prim::Constant[value=9223372036854775807](), scope: __module._represent/__module._represent._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:76:0\n",
      "  %1257 : bool = prim::Constant[value=0](), scope: __module._represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:27:0\n",
      "  %1258 : Device = prim::Constant[value=\"cpu\"](), scope: __module._represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:27:0\n",
      "  %1259 : NoneType = prim::Constant(), scope: __module._represent\n",
      "  %1260 : int = prim::Constant[value=6](), scope: __module._represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:27:0\n",
      "  %1261 : int = prim::Constant[value=128](), scope: __module._represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:27:0\n",
      "  %1262 : int = prim::Constant[value=2](), scope: __module._represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:27:0\n",
      "  %1263 : int = prim::Constant[value=0](), scope: __module._represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:20:0\n",
      "  %_units.3 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"_units\"](%_represent)\n",
      "  %_1.5 : __torch__.fgi.problem.vision.represent.___torch_mangle_7.ImageRepresent = prim::GetAttr[name=\"1\"](%_units.3)\n",
      "  %_units.1 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"_units\"](%_represent)\n",
      "  %_0.3 : __torch__.fgi.problem.vision.represent.ImageRepresent = prim::GetAttr[name=\"0\"](%_units.1)\n",
      "  %1268 : int = aten::size(%x.1, %1263), scope: __module._represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:20:0\n",
      "  %1269 : int[] = prim::ListConstruct(%1262, %1268, %1261), scope: __module._represent\n",
      "  %tempo.1 : Float(2, 1, 128, strides=[128, 128, 1], requires_grad=0, device=cpu) = aten::empty(%1269, %1260, %1259, %1258, %1257, %1259), scope: __module._represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:27:0\n",
      "  %_norm.1 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"_norm\"](%_0.3)\n",
      "  %_attn.1 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"_attn\"](%_0.3)\n",
      "  %_pos_embed.1 : Tensor = prim::GetAttr[name=\"_pos_embed\"](%_0.3)\n",
      "  %_patch_embedding.1 : __torch__.torch.nn.modules.container.Sequential = prim::GetAttr[name=\"_patch_embedding\"](%_0.3)\n",
      "  %_cls_token.1 : Tensor = prim::GetAttr[name=\"_cls_token\"](%_0.3)\n",
      "  %1276 : int = aten::size(%x.1, %1263), scope: __module._represent/__module._represent._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:55:0\n",
      "  %1277 : int[] = prim::ListConstruct(%1276, %1245, %1245), scope: __module._represent/__module._represent._units.0\n",
      "  %cls_token.1 : Float(1, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::expand(%_cls_token.1, %1277, %1257), scope: __module._represent/__module._represent._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:55:0\n",
      "  %_0.1 : __torch__.torch.nn.modules.conv.Conv2d = prim::GetAttr[name=\"0\"](%_patch_embedding.1)\n",
      "  %bias.19 : Tensor = prim::GetAttr[name=\"bias\"](%_0.1)\n",
      "  %weight.19 : Tensor = prim::GetAttr[name=\"weight\"](%_0.1)\n",
      "  %1282 : int[] = prim::ListConstruct(%1246, %1246), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._patch_embedding/__module._represent._units.0._patch_embedding.0\n",
      "  %1283 : int[] = prim::ListConstruct(%1263, %1263), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._patch_embedding/__module._represent._units.0._patch_embedding.0\n",
      "  %1284 : int[] = prim::ListConstruct(%1247, %1247), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._patch_embedding/__module._represent._units.0._patch_embedding.0\n",
      "  %1285 : int[] = prim::ListConstruct(%1263, %1263), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._patch_embedding/__module._represent._units.0._patch_embedding.0\n",
      "  %input.1 : Float(1, 128, 7, 7, strides=[6272, 49, 7, 1], requires_grad=0, device=cpu) = aten::_convolution(%x.1, %weight.19, %bias.19, %1282, %1283, %1284, %1257, %1285, %1247, %1257, %1257, %1248, %1248), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._patch_embedding/__module._represent._units.0._patch_embedding.0 # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549:0\n",
      "  %x.3 : Float(1, 128, 49, strides=[6272, 49, 1], requires_grad=1, device=cpu) = aten::flatten(%input.1, %1262, %1245), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._patch_embedding/__module._represent._units.0._patch_embedding.1 # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\flatten.py:53:0\n",
      "  %1288 : int[] = prim::ListConstruct(%1263, %1262, %1247), scope: __module._represent/__module._represent._units.0\n",
      "  %x.5 : Float(1, 49, 128, strides=[6272, 1, 49], requires_grad=1, device=cpu) = aten::permute(%x.3, %1288), scope: __module._represent/__module._represent._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:59:0\n",
      "  %1290 : Tensor[] = prim::ListConstruct(%cls_token.1, %x.5), scope: __module._represent/__module._represent._units.0\n",
      "  %x.7 : Float(1, 50, 128, strides=[6400, 128, 1], requires_grad=1, device=cpu) = aten::cat(%1290, %1247), scope: __module._represent/__module._represent._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:60:0\n",
      "  %input.3 : Float(1, 50, 128, strides=[6400, 128, 1], requires_grad=1, device=cpu) = aten::add(%_pos_embed.1, %x.7, %1247), scope: __module._represent/__module._represent._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:62:0\n",
      "  %query.1 : Float(1, 50, 128, strides=[6400, 128, 1], requires_grad=1, device=cpu) = aten::dropout(%input.3, %1249, %1257), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._dropout # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n",
      "  %out_proj.3 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%_attn.1)\n",
      "  %bias.21 : Tensor = prim::GetAttr[name=\"bias\"](%out_proj.3)\n",
      "  %out_proj.1 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%_attn.1)\n",
      "  %weight.21 : Tensor = prim::GetAttr[name=\"weight\"](%out_proj.1)\n",
      "  %in_proj_bias.1 : Tensor = prim::GetAttr[name=\"in_proj_bias\"](%_attn.1)\n",
      "  %in_proj_weight.1 : Tensor = prim::GetAttr[name=\"in_proj_weight\"](%_attn.1)\n",
      "  %query.3 : Float(50, 1, 128, strides=[128, 6400, 1], requires_grad=1, device=cpu) = aten::transpose(%query.1, %1247, %1263), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1339:0\n",
      "  %1301 : int = aten::size(%query.3, %1263), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n",
      "  %tgt_len.1 : Long(device=cpu) = prim::NumToTensor(%1301), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %1303 : int = aten::size(%query.3, %1247), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n",
      "  %bsz.1 : Long(device=cpu) = prim::NumToTensor(%1303), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %1305 : int = aten::size(%query.3, %1262), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n",
      "  %embed_dim.1 : Long(device=cpu) = prim::NumToTensor(%1305), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %head_dim.1 : Long(requires_grad=0, device=cpu) = aten::div(%embed_dim.1, %1254, %1253), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6207:0\n",
      "  %1308 : int = aten::Int(%head_dim.1), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %1309 : int = aten::Int(%head_dim.1), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %1310 : int = aten::Int(%head_dim.1), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %1311 : int = aten::size(%query.3, %1245), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5610:0\n",
      "  %1312 : Float(50, 1, 384, strides=[384, 384, 1], requires_grad=1, device=cpu) = aten::linear(%query.3, %in_proj_weight.1, %in_proj_bias.1), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5614:0\n",
      "  %1313 : int[] = prim::ListConstruct(%1252, %1311), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %1314 : Float(50, 1, 3, 128, strides=[384, 384, 128, 1], requires_grad=1, device=cpu) = aten::unflatten(%1312, %1245, %1313), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\_tensor.py:1432:0\n",
      "  %1315 : Float(1, 50, 1, 3, 128, strides=[19200, 384, 384, 128, 1], requires_grad=1, device=cpu) = aten::unsqueeze(%1314, %1263), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5618:0\n",
      "  %1316 : Float(3, 50, 1, 1, 128, strides=[128, 384, 384, 19200, 1], requires_grad=1, device=cpu) = aten::transpose(%1315, %1263, %1251), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5619:0\n",
      "  %1317 : Float(3, 50, 1, 128, strides=[128, 384, 384, 1], requires_grad=1, device=cpu) = aten::squeeze(%1316, %1251), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5620:0\n",
      "  %proj.1 : Float(3, 50, 1, 128, strides=[6400, 128, 128, 1], requires_grad=1, device=cpu) = aten::contiguous(%1317, %1263), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5621:0\n",
      "  %q.1 : Float(50, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::select(%proj.1, %1263, %1263), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n",
      "  %k.1 : Float(50, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::select(%proj.1, %1263, %1247), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n",
      "  %v.1 : Float(50, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::select(%proj.1, %1263, %1262), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n",
      "  %1322 : Long(requires_grad=0, device=cpu) = aten::mul(%bsz.1, %1254), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n",
      "  %1323 : int = aten::Int(%1322), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %1324 : int[] = prim::ListConstruct(%1301, %1323, %1310), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %1325 : Float(50, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::view(%q.1, %1324), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n",
      "  %q.3 : Float(1, 50, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::transpose(%1325, %1263, %1247), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n",
      "  %1327 : int = aten::size(%k.1, %1263), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n",
      "  %1328 : Long(requires_grad=0, device=cpu) = aten::mul(%bsz.1, %1254), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n",
      "  %1329 : int = aten::Int(%1328), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %1330 : int[] = prim::ListConstruct(%1327, %1329, %1309), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %1331 : Float(50, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::view(%k.1, %1330), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n",
      "  %k.3 : Float(1, 50, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::transpose(%1331, %1263, %1247), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n",
      "  %1333 : int = aten::size(%v.1, %1263), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n",
      "  %1334 : Long(requires_grad=0, device=cpu) = aten::mul(%bsz.1, %1254), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n",
      "  %1335 : int = aten::Int(%1334), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %1336 : int[] = prim::ListConstruct(%1333, %1335, %1308), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %1337 : Float(50, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::view(%v.1, %1336), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n",
      "  %v.3 : Float(1, 50, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::transpose(%1337, %1263, %1247), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n",
      "  %q_scaled.1 : Float(1, 50, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::mul(%q.3, %1250), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6362:0\n",
      "  %1340 : Float(1, 128, 50, strides=[128, 1, 128], requires_grad=1, device=cpu) = aten::transpose(%k.3, %1251, %1245), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6373:0\n",
      "  %input.5 : Float(1, 50, 50, strides=[2500, 50, 1], requires_grad=1, device=cpu) = aten::bmm(%q_scaled.1, %1340), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6373:0\n",
      "  %attn_output_weights.1 : Float(1, 50, 50, strides=[2500, 50, 1], requires_grad=1, device=cpu) = aten::softmax(%input.5, %1245, %1259), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:2140:0\n",
      "  %attn_output.1 : Float(1, 50, 128, strides=[6400, 128, 1], requires_grad=1, device=cpu) = aten::bmm(%attn_output_weights.1, %v.3), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6378:0\n",
      "  %1344 : Float(50, 1, 128, strides=[128, 6400, 1], requires_grad=1, device=cpu) = aten::transpose(%attn_output.1, %1263, %1247), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6381:0\n",
      "  %1345 : Float(50, 1, 128, strides=[128, 6400, 1], requires_grad=1, device=cpu) = aten::contiguous(%1344, %1263), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6381:0\n",
      "  %1346 : Long(requires_grad=0, device=cpu) = aten::mul(%tgt_len.1, %bsz.1), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6381:0\n",
      "  %1347 : int = aten::Int(%1346), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %1348 : int[] = prim::ListConstruct(%1347, %1305), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %attn_output.3 : Float(50, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::view(%1345, %1348), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6381:0\n",
      "  %attn_output.5 : Float(50, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::linear(%attn_output.3, %weight.21, %bias.21), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6383:0\n",
      "  %1351 : int = aten::size(%attn_output.5, %1247), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6384:0\n",
      "  %1352 : int[] = prim::ListConstruct(%1301, %1303, %1351), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn\n",
      "  %attn_output.7 : Float(50, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::view(%attn_output.5, %1352), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6384:0\n",
      "  %input.7 : Float(1, 50, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::transpose(%attn_output.7, %1247, %1263), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1395:0\n",
      "  %bias.23 : Tensor = prim::GetAttr[name=\"bias\"](%_norm.1)\n",
      "  %weight.23 : Tensor = prim::GetAttr[name=\"weight\"](%_norm.1)\n",
      "  %1357 : int[] = prim::ListConstruct(%1261), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._norm\n",
      "  %x.9 : Float(1, 50, 128, strides=[6400, 128, 1], requires_grad=1, device=cpu) = aten::layer_norm(%input.7, %1357, %weight.23, %bias.23, %1255, %1248), scope: __module._represent/__module._represent._units.0/__module._represent._units.0._norm # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n",
      "  %1359 : Float(1, 50, 128, strides=[6400, 128, 1], requires_grad=1, device=cpu) = aten::slice(%x.9, %1263, %1263, %1256, %1247), scope: __module._represent/__module._represent._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:76:0\n",
      "  %1360 : Float(1, 128, strides=[6400, 1], requires_grad=1, device=cpu) = aten::select(%1359, %1247, %1263), scope: __module._represent/__module._represent._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:76:0\n",
      "  %1361 : Float(1, 128, strides=[128, 1], requires_grad=0, device=cpu) = aten::select(%tempo.1, %1263, %1263), scope: __module._represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %1362 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::copy_(%1361, %1360, %1257), scope: __module._represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %_norm.3 : __torch__.torch.nn.modules.normalization.___torch_mangle_6.LayerNorm = prim::GetAttr[name=\"_norm\"](%_1.5)\n",
      "  %_attn : __torch__.torch.nn.modules.activation.___torch_mangle_5.MultiheadAttention = prim::GetAttr[name=\"_attn\"](%_1.5)\n",
      "  %_pos_embed : Tensor = prim::GetAttr[name=\"_pos_embed\"](%_1.5)\n",
      "  %_patch_embedding : __torch__.torch.nn.modules.container.___torch_mangle_2.Sequential = prim::GetAttr[name=\"_patch_embedding\"](%_1.5)\n",
      "  %_cls_token : Tensor = prim::GetAttr[name=\"_cls_token\"](%_1.5)\n",
      "  %1368 : int = aten::size(%x.1, %1263), scope: __module._represent/__module._represent._units.1 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:55:0\n",
      "  %1369 : int[] = prim::ListConstruct(%1368, %1245, %1245), scope: __module._represent/__module._represent._units.1\n",
      "  %cls_token : Float(1, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::expand(%_cls_token, %1369, %1257), scope: __module._represent/__module._represent._units.1 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:55:0\n",
      "  %_0.5 : __torch__.torch.nn.modules.conv.___torch_mangle_0.Conv2d = prim::GetAttr[name=\"0\"](%_patch_embedding)\n",
      "  %bias.25 : Tensor = prim::GetAttr[name=\"bias\"](%_0.5)\n",
      "  %weight.25 : Tensor = prim::GetAttr[name=\"weight\"](%_0.5)\n",
      "  %1374 : int[] = prim::ListConstruct(%1246, %1246), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._patch_embedding/__module._represent._units.1._patch_embedding.0\n",
      "  %1375 : int[] = prim::ListConstruct(%1263, %1263), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._patch_embedding/__module._represent._units.1._patch_embedding.0\n",
      "  %1376 : int[] = prim::ListConstruct(%1247, %1247), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._patch_embedding/__module._represent._units.1._patch_embedding.0\n",
      "  %1377 : int[] = prim::ListConstruct(%1263, %1263), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._patch_embedding/__module._represent._units.1._patch_embedding.0\n",
      "  %input.9 : Float(1, 128, 7, 7, strides=[6272, 49, 7, 1], requires_grad=0, device=cpu) = aten::_convolution(%x.1, %weight.25, %bias.25, %1374, %1375, %1376, %1257, %1377, %1247, %1257, %1257, %1248, %1248), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._patch_embedding/__module._represent._units.1._patch_embedding.0 # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549:0\n",
      "  %x.11 : Float(1, 128, 49, strides=[6272, 49, 1], requires_grad=1, device=cpu) = aten::flatten(%input.9, %1262, %1245), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._patch_embedding/__module._represent._units.1._patch_embedding.1 # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\flatten.py:53:0\n",
      "  %1380 : int[] = prim::ListConstruct(%1263, %1262, %1247), scope: __module._represent/__module._represent._units.1\n",
      "  %x.13 : Float(1, 49, 128, strides=[6272, 1, 49], requires_grad=1, device=cpu) = aten::permute(%x.11, %1380), scope: __module._represent/__module._represent._units.1 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:59:0\n",
      "  %1382 : Tensor[] = prim::ListConstruct(%cls_token, %x.13), scope: __module._represent/__module._represent._units.1\n",
      "  %x.15 : Float(1, 50, 128, strides=[6400, 128, 1], requires_grad=1, device=cpu) = aten::cat(%1382, %1247), scope: __module._represent/__module._represent._units.1 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:60:0\n",
      "  %input.11 : Float(1, 50, 128, strides=[6400, 128, 1], requires_grad=1, device=cpu) = aten::add(%_pos_embed, %x.15, %1247), scope: __module._represent/__module._represent._units.1 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:62:0\n",
      "  %query.5 : Float(1, 50, 128, strides=[6400, 128, 1], requires_grad=1, device=cpu) = aten::dropout(%input.11, %1249, %1257), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._dropout # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n",
      "  %out_proj : __torch__.torch.nn.modules.linear.___torch_mangle_4.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%_attn)\n",
      "  %bias.27 : Tensor = prim::GetAttr[name=\"bias\"](%out_proj)\n",
      "  %out_proj.5 : __torch__.torch.nn.modules.linear.___torch_mangle_4.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%_attn)\n",
      "  %weight.27 : Tensor = prim::GetAttr[name=\"weight\"](%out_proj.5)\n",
      "  %in_proj_bias : Tensor = prim::GetAttr[name=\"in_proj_bias\"](%_attn)\n",
      "  %in_proj_weight : Tensor = prim::GetAttr[name=\"in_proj_weight\"](%_attn)\n",
      "  %query : Float(50, 1, 128, strides=[128, 6400, 1], requires_grad=1, device=cpu) = aten::transpose(%query.5, %1247, %1263), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1339:0\n",
      "  %1393 : int = aten::size(%query, %1263), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n",
      "  %tgt_len : Long(device=cpu) = prim::NumToTensor(%1393), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %1395 : int = aten::size(%query, %1247), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n",
      "  %bsz : Long(device=cpu) = prim::NumToTensor(%1395), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %1397 : int = aten::size(%query, %1262), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n",
      "  %embed_dim : Long(device=cpu) = prim::NumToTensor(%1397), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %head_dim : Long(requires_grad=0, device=cpu) = aten::div(%embed_dim, %1254, %1253), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6207:0\n",
      "  %1400 : int = aten::Int(%head_dim), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %1401 : int = aten::Int(%head_dim), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %1402 : int = aten::Int(%head_dim), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %1403 : int = aten::size(%query, %1245), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5610:0\n",
      "  %1404 : Float(50, 1, 384, strides=[384, 384, 1], requires_grad=1, device=cpu) = aten::linear(%query, %in_proj_weight, %in_proj_bias), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5614:0\n",
      "  %1405 : int[] = prim::ListConstruct(%1252, %1403), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %1406 : Float(50, 1, 3, 128, strides=[384, 384, 128, 1], requires_grad=1, device=cpu) = aten::unflatten(%1404, %1245, %1405), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\_tensor.py:1432:0\n",
      "  %1407 : Float(1, 50, 1, 3, 128, strides=[19200, 384, 384, 128, 1], requires_grad=1, device=cpu) = aten::unsqueeze(%1406, %1263), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5618:0\n",
      "  %1408 : Float(3, 50, 1, 1, 128, strides=[128, 384, 384, 19200, 1], requires_grad=1, device=cpu) = aten::transpose(%1407, %1263, %1251), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5619:0\n",
      "  %1409 : Float(3, 50, 1, 128, strides=[128, 384, 384, 1], requires_grad=1, device=cpu) = aten::squeeze(%1408, %1251), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5620:0\n",
      "  %proj : Float(3, 50, 1, 128, strides=[6400, 128, 128, 1], requires_grad=1, device=cpu) = aten::contiguous(%1409, %1263), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5621:0\n",
      "  %q.5 : Float(50, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::select(%proj, %1263, %1263), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n",
      "  %k.5 : Float(50, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::select(%proj, %1263, %1247), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n",
      "  %v.5 : Float(50, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::select(%proj, %1263, %1262), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n",
      "  %1414 : Long(requires_grad=0, device=cpu) = aten::mul(%bsz, %1254), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n",
      "  %1415 : int = aten::Int(%1414), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %1416 : int[] = prim::ListConstruct(%1393, %1415, %1402), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %1417 : Float(50, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::view(%q.5, %1416), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n",
      "  %q : Float(1, 50, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::transpose(%1417, %1263, %1247), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n",
      "  %1419 : int = aten::size(%k.5, %1263), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n",
      "  %1420 : Long(requires_grad=0, device=cpu) = aten::mul(%bsz, %1254), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n",
      "  %1421 : int = aten::Int(%1420), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %1422 : int[] = prim::ListConstruct(%1419, %1421, %1401), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %1423 : Float(50, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::view(%k.5, %1422), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n",
      "  %k : Float(1, 50, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::transpose(%1423, %1263, %1247), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n",
      "  %1425 : int = aten::size(%v.5, %1263), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n",
      "  %1426 : Long(requires_grad=0, device=cpu) = aten::mul(%bsz, %1254), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n",
      "  %1427 : int = aten::Int(%1426), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %1428 : int[] = prim::ListConstruct(%1425, %1427, %1400), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %1429 : Float(50, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::view(%v.5, %1428), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n",
      "  %v : Float(1, 50, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::transpose(%1429, %1263, %1247), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n",
      "  %q_scaled : Float(1, 50, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::mul(%q, %1250), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6362:0\n",
      "  %1432 : Float(1, 128, 50, strides=[128, 1, 128], requires_grad=1, device=cpu) = aten::transpose(%k, %1251, %1245), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6373:0\n",
      "  %input.13 : Float(1, 50, 50, strides=[2500, 50, 1], requires_grad=1, device=cpu) = aten::bmm(%q_scaled, %1432), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6373:0\n",
      "  %attn_output_weights.5 : Float(1, 50, 50, strides=[2500, 50, 1], requires_grad=1, device=cpu) = aten::softmax(%input.13, %1245, %1259), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:2140:0\n",
      "  %attn_output.9 : Float(1, 50, 128, strides=[6400, 128, 1], requires_grad=1, device=cpu) = aten::bmm(%attn_output_weights.5, %v), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6378:0\n",
      "  %1436 : Float(50, 1, 128, strides=[128, 6400, 1], requires_grad=1, device=cpu) = aten::transpose(%attn_output.9, %1263, %1247), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6381:0\n",
      "  %1437 : Float(50, 1, 128, strides=[128, 6400, 1], requires_grad=1, device=cpu) = aten::contiguous(%1436, %1263), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6381:0\n",
      "  %1438 : Long(requires_grad=0, device=cpu) = aten::mul(%tgt_len, %bsz), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6381:0\n",
      "  %1439 : int = aten::Int(%1438), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %1440 : int[] = prim::ListConstruct(%1439, %1397), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %attn_output.11 : Float(50, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::view(%1437, %1440), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6381:0\n",
      "  %attn_output.13 : Float(50, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::linear(%attn_output.11, %weight.27, %bias.27), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6383:0\n",
      "  %1443 : int = aten::size(%attn_output.13, %1247), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6384:0\n",
      "  %1444 : int[] = prim::ListConstruct(%1393, %1395, %1443), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn\n",
      "  %attn_output : Float(50, 1, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::view(%attn_output.13, %1444), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:6384:0\n",
      "  %input.15 : Float(1, 50, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::transpose(%attn_output, %1247, %1263), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._attn # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1395:0\n",
      "  %bias.29 : Tensor = prim::GetAttr[name=\"bias\"](%_norm.3)\n",
      "  %weight.29 : Tensor = prim::GetAttr[name=\"weight\"](%_norm.3)\n",
      "  %1449 : int[] = prim::ListConstruct(%1261), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._norm\n",
      "  %x.17 : Float(1, 50, 128, strides=[6400, 128, 1], requires_grad=1, device=cpu) = aten::layer_norm(%input.15, %1449, %weight.29, %bias.29, %1255, %1248), scope: __module._represent/__module._represent._units.1/__module._represent._units.1._norm # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n",
      "  %1451 : Float(1, 50, 128, strides=[6400, 128, 1], requires_grad=1, device=cpu) = aten::slice(%x.17, %1263, %1263, %1256, %1247), scope: __module._represent/__module._represent._units.1 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:76:0\n",
      "  %1452 : Float(1, 128, strides=[6400, 1], requires_grad=1, device=cpu) = aten::select(%1451, %1247, %1263), scope: __module._represent/__module._represent._units.1 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\problem\\vision\\represent.py:76:0\n",
      "  %1453 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::select(%tempo.1, %1263, %1247), scope: __module._represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %1454 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::copy_(%1453, %1452, %1257), scope: __module._represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %x.19 : Float(1, 2, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::transpose(%tempo.1, %1263, %1247), scope: __module._represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:32:0\n",
      "  %1456 : float = prim::Constant[value=0.20000000000000001](), scope: __module._co_represent/__module._co_represent._units.0/__module._co_represent._units.0._memory/__module._co_represent._units.0._memory._drop # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n",
      "  %1457 : int = prim::Constant[value=1](), scope: __module._co_represent/__module._co_represent._units.0/__module._co_represent._units.0._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:36:0\n",
      "  %1458 : bool = prim::Constant[value=0](), scope: __module._co_represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:27:0\n",
      "  %1459 : Device = prim::Constant[value=\"cpu\"](), scope: __module._co_represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:27:0\n",
      "  %1460 : NoneType = prim::Constant(), scope: __module._co_represent\n",
      "  %1461 : int = prim::Constant[value=6](), scope: __module._co_represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:27:0\n",
      "  %1462 : int = prim::Constant[value=2](), scope: __module._co_represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:22:0\n",
      "  %1463 : int = prim::Constant[value=0](), scope: __module._co_represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:20:0\n",
      "  %_units.7 : __torch__.torch.nn.modules.container.___torch_mangle_14.ModuleList = prim::GetAttr[name=\"_units\"](%_co_represent)\n",
      "  %_1.7 : __torch__.fgi.units.co_represent.___torch_mangle_13.CoRepresentUnit = prim::GetAttr[name=\"1\"](%_units.7)\n",
      "  %_units.5 : __torch__.torch.nn.modules.container.___torch_mangle_14.ModuleList = prim::GetAttr[name=\"_units\"](%_co_represent)\n",
      "  %_0.7 : __torch__.fgi.units.co_represent.CoRepresentUnit = prim::GetAttr[name=\"0\"](%_units.5)\n",
      "  %1468 : int = aten::size(%x.19, %1463), scope: __module._co_represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:20:0\n",
      "  %1469 : int = aten::size(%x.19, %1462), scope: __module._co_represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:22:0\n",
      "  %1470 : int[] = prim::ListConstruct(%1462, %1468, %1469), scope: __module._co_represent\n",
      "  %tempo.3 : Float(2, 1, 128, strides=[128, 128, 1], requires_grad=0, device=cpu) = aten::empty(%1470, %1461, %1460, %1459, %1458, %1460), scope: __module._co_represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:27:0\n",
      "  %_combine.1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"_combine\"](%_0.7)\n",
      "  %_memory.1 : __torch__.fgi.units.memory.SoftMemoryUnit = prim::GetAttr[name=\"_memory\"](%_0.7)\n",
      "  %_weighted.1 : Tensor = prim::GetAttr[name=\"_weighted\"](%_0.7)\n",
      "  %weighted.1 : Float(2, strides=[1], requires_grad=1, device=cpu) = aten::softmax(%_weighted.1, %1463, %1460), scope: __module._co_represent/__module._co_represent._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\co_represent.py:30:0\n",
      "  %x.21 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::matmul(%weighted.1, %x.19), scope: __module._co_represent/__module._co_represent._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\co_represent.py:31:0\n",
      "  %_patterns.1 : Tensor = prim::GetAttr[name=\"_patterns\"](%_memory.1)\n",
      "  %_beta.1 : Tensor = prim::GetAttr[name=\"_beta\"](%_memory.1)\n",
      "  %1479 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::mul(%_beta.1, %x.21), scope: __module._co_represent/__module._co_represent._units.0/__module._co_represent._units.0._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %1480 : Float(128, 128, strides=[1, 128], requires_grad=1, device=cpu) = aten::numpy_T(%_patterns.1), scope: __module._co_represent/__module._co_represent._units.0/__module._co_represent._units.0._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %weighted.3 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::matmul(%1479, %1480), scope: __module._co_represent/__module._co_represent._units.0/__module._co_represent._units.0._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %weighted.5 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::softmax(%weighted.3, %1457, %1460), scope: __module._co_represent/__module._co_represent._units.0/__module._co_represent._units.0._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:36:0\n",
      "  %input.17 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::matmul(%weighted.5, %_patterns.1), scope: __module._co_represent/__module._co_represent._units.0/__module._co_represent._units.0._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:37:0\n",
      "  %p.1 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::dropout(%input.17, %1456, %1458), scope: __module._co_represent/__module._co_represent._units.0/__module._co_represent._units.0._memory/__module._co_represent._units.0._memory._drop # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n",
      "  %1485 : Tensor[] = prim::ListConstruct(%x.21, %p.1), scope: __module._co_represent/__module._co_represent._units.0\n",
      "  %input.19 : Float(1, 256, strides=[256, 1], requires_grad=1, device=cpu) = aten::cat(%1485, %1457), scope: __module._co_represent/__module._co_represent._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\co_represent.py:37:0\n",
      "  %bias.31 : Tensor = prim::GetAttr[name=\"bias\"](%_combine.1)\n",
      "  %weight.31 : Tensor = prim::GetAttr[name=\"weight\"](%_combine.1)\n",
      "  %input.21 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::linear(%input.19, %weight.31, %bias.31), scope: __module._co_represent/__module._co_represent._units.0/__module._co_represent._units.0._combine # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n",
      "  %1490 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::relu(%input.21), scope: __module._co_represent/__module._co_represent._units.0/__module._co_represent._units.0._activate # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:1704:0\n",
      "  %1491 : Float(1, 128, strides=[128, 1], requires_grad=0, device=cpu) = aten::select(%tempo.3, %1463, %1463), scope: __module._co_represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %1492 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::copy_(%1491, %1490, %1458), scope: __module._co_represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %_combine : __torch__.torch.nn.modules.linear.___torch_mangle_11.Linear = prim::GetAttr[name=\"_combine\"](%_1.7)\n",
      "  %_memory.3 : __torch__.fgi.units.memory.___torch_mangle_10.SoftMemoryUnit = prim::GetAttr[name=\"_memory\"](%_1.7)\n",
      "  %_weighted.3 : Tensor = prim::GetAttr[name=\"_weighted\"](%_1.7)\n",
      "  %weighted.7 : Float(2, strides=[1], requires_grad=1, device=cpu) = aten::softmax(%_weighted.3, %1463, %1460), scope: __module._co_represent/__module._co_represent._units.1 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\co_represent.py:30:0\n",
      "  %x.23 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::matmul(%weighted.7, %x.19), scope: __module._co_represent/__module._co_represent._units.1 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\co_represent.py:31:0\n",
      "  %_patterns.3 : Tensor = prim::GetAttr[name=\"_patterns\"](%_memory.3)\n",
      "  %_beta.3 : Tensor = prim::GetAttr[name=\"_beta\"](%_memory.3)\n",
      "  %1500 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::mul(%_beta.3, %x.23), scope: __module._co_represent/__module._co_represent._units.1/__module._co_represent._units.1._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %1501 : Float(128, 128, strides=[1, 128], requires_grad=1, device=cpu) = aten::numpy_T(%_patterns.3), scope: __module._co_represent/__module._co_represent._units.1/__module._co_represent._units.1._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %weighted.9 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::matmul(%1500, %1501), scope: __module._co_represent/__module._co_represent._units.1/__module._co_represent._units.1._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %weighted.11 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::softmax(%weighted.9, %1457, %1460), scope: __module._co_represent/__module._co_represent._units.1/__module._co_represent._units.1._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:36:0\n",
      "  %input.23 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::matmul(%weighted.11, %_patterns.3), scope: __module._co_represent/__module._co_represent._units.1/__module._co_represent._units.1._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:37:0\n",
      "  %p.3 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::dropout(%input.23, %1456, %1458), scope: __module._co_represent/__module._co_represent._units.1/__module._co_represent._units.1._memory/__module._co_represent._units.1._memory._drop # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n",
      "  %1506 : Tensor[] = prim::ListConstruct(%x.23, %p.3), scope: __module._co_represent/__module._co_represent._units.1\n",
      "  %input.25 : Float(1, 256, strides=[256, 1], requires_grad=1, device=cpu) = aten::cat(%1506, %1457), scope: __module._co_represent/__module._co_represent._units.1 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\co_represent.py:37:0\n",
      "  %bias.33 : Tensor = prim::GetAttr[name=\"bias\"](%_combine)\n",
      "  %weight.33 : Tensor = prim::GetAttr[name=\"weight\"](%_combine)\n",
      "  %input.27 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::linear(%input.25, %weight.33, %bias.33), scope: __module._co_represent/__module._co_represent._units.1/__module._co_represent._units.1._combine # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n",
      "  %1511 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::relu(%input.27), scope: __module._co_represent/__module._co_represent._units.1/__module._co_represent._units.1._activate # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:1704:0\n",
      "  %1512 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::select(%tempo.3, %1463, %1457), scope: __module._co_represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %1513 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::copy_(%1512, %1511, %1458), scope: __module._co_represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %x.25 : Float(1, 2, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::transpose(%tempo.3, %1463, %1457), scope: __module._co_represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:32:0\n",
      "  %1515 : int[] = prim::ListConstruct(%1457), scope: __module._co_represent\n",
      "  %x.27 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::mean(%x.25, %1515, %1458, %1460), scope: __module._co_represent # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\CoRepresentLayer.py:15:0\n",
      "  %1517 : int = prim::Constant[value=3](), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %1518 : int = prim::Constant[value=2](), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %1519 : float = prim::Constant[value=0.20000000000000001](), scope: __module._property/__module._property._units.0/__module._property._units.0._memory/__module._property._units.0._memory._drop # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n",
      "  %1520 : bool = prim::Constant[value=1](), scope: __module._property/__module._property._units.0/__module._property._units.0._norm # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n",
      "  %1521 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module._property/__module._property._units.0/__module._property._units.0._norm # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n",
      "  %1522 : int = prim::Constant[value=128](), scope: __module._property/__module._property._units.0/__module._property._units.0._norm # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n",
      "  %1523 : bool = prim::Constant[value=0](), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:27:0\n",
      "  %1524 : Device = prim::Constant[value=\"cpu\"](), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:27:0\n",
      "  %1525 : NoneType = prim::Constant(), scope: __module._property\n",
      "  %1526 : int = prim::Constant[value=6](), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:27:0\n",
      "  %1527 : int = prim::Constant[value=4](), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:27:0\n",
      "  %1528 : int = prim::Constant[value=1](), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:22:0\n",
      "  %1529 : int = prim::Constant[value=0](), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:20:0\n",
      "  %_units.15 : __torch__.torch.nn.modules.container.___torch_mangle_34.ModuleList = prim::GetAttr[name=\"_units\"](%_property)\n",
      "  %_3 : __torch__.fgi.units.property.___torch_mangle_33.PropertyUnit = prim::GetAttr[name=\"3\"](%_units.15)\n",
      "  %_units.13 : __torch__.torch.nn.modules.container.___torch_mangle_34.ModuleList = prim::GetAttr[name=\"_units\"](%_property)\n",
      "  %_2 : __torch__.fgi.units.property.___torch_mangle_28.PropertyUnit = prim::GetAttr[name=\"2\"](%_units.13)\n",
      "  %_units.11 : __torch__.torch.nn.modules.container.___torch_mangle_34.ModuleList = prim::GetAttr[name=\"_units\"](%_property)\n",
      "  %_1.9 : __torch__.fgi.units.property.___torch_mangle_23.PropertyUnit = prim::GetAttr[name=\"1\"](%_units.11)\n",
      "  %_units.9 : __torch__.torch.nn.modules.container.___torch_mangle_34.ModuleList = prim::GetAttr[name=\"_units\"](%_property)\n",
      "  %_0.9 : __torch__.fgi.units.property.PropertyUnit = prim::GetAttr[name=\"0\"](%_units.9)\n",
      "  %1538 : int = aten::size(%x.27, %1529), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:20:0\n",
      "  %1539 : int = aten::size(%x.27, %1528), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:22:0\n",
      "  %1540 : int[] = prim::ListConstruct(%1527, %1538, %1539), scope: __module._property\n",
      "  %tempo : Float(4, 1, 128, strides=[128, 128, 1], requires_grad=0, device=cpu) = aten::empty(%1540, %1526, %1525, %1524, %1523, %1525), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:27:0\n",
      "  %_norm.5 : __torch__.torch.nn.modules.normalization.___torch_mangle_18.LayerNorm = prim::GetAttr[name=\"_norm\"](%_0.9)\n",
      "  %_evaluation.1 : __torch__.torch.nn.modules.linear.___torch_mangle_17.Linear = prim::GetAttr[name=\"_evaluation\"](%_0.9)\n",
      "  %_memory.5 : __torch__.fgi.units.memory.___torch_mangle_16.SoftMemoryUnit = prim::GetAttr[name=\"_memory\"](%_0.9)\n",
      "  %_patterns.5 : Tensor = prim::GetAttr[name=\"_patterns\"](%_memory.5)\n",
      "  %_beta.5 : Tensor = prim::GetAttr[name=\"_beta\"](%_memory.5)\n",
      "  %1547 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::mul(%_beta.5, %x.27), scope: __module._property/__module._property._units.0/__module._property._units.0._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %1548 : Float(128, 128, strides=[1, 128], requires_grad=1, device=cpu) = aten::numpy_T(%_patterns.5), scope: __module._property/__module._property._units.0/__module._property._units.0._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %weighted.13 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::matmul(%1547, %1548), scope: __module._property/__module._property._units.0/__module._property._units.0._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %weighted.15 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::softmax(%weighted.13, %1528, %1525), scope: __module._property/__module._property._units.0/__module._property._units.0._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:36:0\n",
      "  %input.29 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::matmul(%weighted.15, %_patterns.5), scope: __module._property/__module._property._units.0/__module._property._units.0._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:37:0\n",
      "  %p.5 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::dropout(%input.29, %1519, %1523), scope: __module._property/__module._property._units.0/__module._property._units.0._memory/__module._property._units.0._memory._drop # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n",
      "  %bias.35 : Tensor = prim::GetAttr[name=\"bias\"](%_evaluation.1)\n",
      "  %weight.35 : Tensor = prim::GetAttr[name=\"weight\"](%_evaluation.1)\n",
      "  %z.1 : Float(1, 1, strides=[1, 1], requires_grad=1, device=cpu) = aten::linear(%x.27, %weight.35, %bias.35), scope: __module._property/__module._property._units.0/__module._property._units.0._evaluation # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n",
      "  %z.3 : Float(1, 1, strides=[1, 1], requires_grad=1, device=cpu) = aten::sigmoid(%z.1), scope: __module._property/__module._property._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\property.py:22:0\n",
      "  %input.31 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::mul(%p.5, %z.3), scope: __module._property/__module._property._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\property.py:25:0\n",
      "  %bias.37 : Tensor = prim::GetAttr[name=\"bias\"](%_norm.5)\n",
      "  %weight.37 : Tensor = prim::GetAttr[name=\"weight\"](%_norm.5)\n",
      "  %1560 : int[] = prim::ListConstruct(%1522), scope: __module._property/__module._property._units.0/__module._property._units.0._norm\n",
      "  %1561 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::layer_norm(%input.31, %1560, %weight.37, %bias.37, %1521, %1520), scope: __module._property/__module._property._units.0/__module._property._units.0._norm # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n",
      "  %1562 : Float(1, 128, strides=[128, 1], requires_grad=0, device=cpu) = aten::select(%tempo, %1529, %1529), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %1563 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::copy_(%1562, %1561, %1523), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %_norm.7 : __torch__.torch.nn.modules.normalization.___torch_mangle_22.LayerNorm = prim::GetAttr[name=\"_norm\"](%_1.9)\n",
      "  %_evaluation.3 : __torch__.torch.nn.modules.linear.___torch_mangle_21.Linear = prim::GetAttr[name=\"_evaluation\"](%_1.9)\n",
      "  %_memory.7 : __torch__.fgi.units.memory.___torch_mangle_20.SoftMemoryUnit = prim::GetAttr[name=\"_memory\"](%_1.9)\n",
      "  %_patterns.7 : Tensor = prim::GetAttr[name=\"_patterns\"](%_memory.7)\n",
      "  %_beta.7 : Tensor = prim::GetAttr[name=\"_beta\"](%_memory.7)\n",
      "  %1569 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::mul(%_beta.7, %x.27), scope: __module._property/__module._property._units.1/__module._property._units.1._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %1570 : Float(128, 128, strides=[1, 128], requires_grad=1, device=cpu) = aten::numpy_T(%_patterns.7), scope: __module._property/__module._property._units.1/__module._property._units.1._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %weighted.17 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::matmul(%1569, %1570), scope: __module._property/__module._property._units.1/__module._property._units.1._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %weighted.19 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::softmax(%weighted.17, %1528, %1525), scope: __module._property/__module._property._units.1/__module._property._units.1._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:36:0\n",
      "  %input.33 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::matmul(%weighted.19, %_patterns.7), scope: __module._property/__module._property._units.1/__module._property._units.1._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:37:0\n",
      "  %p.7 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::dropout(%input.33, %1519, %1523), scope: __module._property/__module._property._units.1/__module._property._units.1._memory/__module._property._units.1._memory._drop # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n",
      "  %bias.39 : Tensor = prim::GetAttr[name=\"bias\"](%_evaluation.3)\n",
      "  %weight.39 : Tensor = prim::GetAttr[name=\"weight\"](%_evaluation.3)\n",
      "  %z.5 : Float(1, 1, strides=[1, 1], requires_grad=1, device=cpu) = aten::linear(%x.27, %weight.39, %bias.39), scope: __module._property/__module._property._units.1/__module._property._units.1._evaluation # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n",
      "  %z.7 : Float(1, 1, strides=[1, 1], requires_grad=1, device=cpu) = aten::sigmoid(%z.5), scope: __module._property/__module._property._units.1 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\property.py:22:0\n",
      "  %input.35 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::mul(%p.7, %z.7), scope: __module._property/__module._property._units.1 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\property.py:25:0\n",
      "  %bias.41 : Tensor = prim::GetAttr[name=\"bias\"](%_norm.7)\n",
      "  %weight.41 : Tensor = prim::GetAttr[name=\"weight\"](%_norm.7)\n",
      "  %1582 : int[] = prim::ListConstruct(%1522), scope: __module._property/__module._property._units.1/__module._property._units.1._norm\n",
      "  %1583 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::layer_norm(%input.35, %1582, %weight.41, %bias.41, %1521, %1520), scope: __module._property/__module._property._units.1/__module._property._units.1._norm # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n",
      "  %1584 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::select(%tempo, %1529, %1528), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %1585 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::copy_(%1584, %1583, %1523), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %_norm.9 : __torch__.torch.nn.modules.normalization.___torch_mangle_27.LayerNorm = prim::GetAttr[name=\"_norm\"](%_2)\n",
      "  %_evaluation.5 : __torch__.torch.nn.modules.linear.___torch_mangle_26.Linear = prim::GetAttr[name=\"_evaluation\"](%_2)\n",
      "  %_memory.9 : __torch__.fgi.units.memory.___torch_mangle_25.SoftMemoryUnit = prim::GetAttr[name=\"_memory\"](%_2)\n",
      "  %_patterns.9 : Tensor = prim::GetAttr[name=\"_patterns\"](%_memory.9)\n",
      "  %_beta.9 : Tensor = prim::GetAttr[name=\"_beta\"](%_memory.9)\n",
      "  %1591 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::mul(%_beta.9, %x.27), scope: __module._property/__module._property._units.2/__module._property._units.2._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %1592 : Float(128, 128, strides=[1, 128], requires_grad=1, device=cpu) = aten::numpy_T(%_patterns.9), scope: __module._property/__module._property._units.2/__module._property._units.2._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %weighted.21 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::matmul(%1591, %1592), scope: __module._property/__module._property._units.2/__module._property._units.2._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %weighted.23 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::softmax(%weighted.21, %1528, %1525), scope: __module._property/__module._property._units.2/__module._property._units.2._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:36:0\n",
      "  %input.37 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::matmul(%weighted.23, %_patterns.9), scope: __module._property/__module._property._units.2/__module._property._units.2._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:37:0\n",
      "  %p.9 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::dropout(%input.37, %1519, %1523), scope: __module._property/__module._property._units.2/__module._property._units.2._memory/__module._property._units.2._memory._drop # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n",
      "  %bias.43 : Tensor = prim::GetAttr[name=\"bias\"](%_evaluation.5)\n",
      "  %weight.43 : Tensor = prim::GetAttr[name=\"weight\"](%_evaluation.5)\n",
      "  %z.9 : Float(1, 1, strides=[1, 1], requires_grad=1, device=cpu) = aten::linear(%x.27, %weight.43, %bias.43), scope: __module._property/__module._property._units.2/__module._property._units.2._evaluation # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n",
      "  %z.11 : Float(1, 1, strides=[1, 1], requires_grad=1, device=cpu) = aten::sigmoid(%z.9), scope: __module._property/__module._property._units.2 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\property.py:22:0\n",
      "  %input.39 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::mul(%p.9, %z.11), scope: __module._property/__module._property._units.2 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\property.py:25:0\n",
      "  %bias.45 : Tensor = prim::GetAttr[name=\"bias\"](%_norm.9)\n",
      "  %weight.45 : Tensor = prim::GetAttr[name=\"weight\"](%_norm.9)\n",
      "  %1604 : int[] = prim::ListConstruct(%1522), scope: __module._property/__module._property._units.2/__module._property._units.2._norm\n",
      "  %1605 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::layer_norm(%input.39, %1604, %weight.45, %bias.45, %1521, %1520), scope: __module._property/__module._property._units.2/__module._property._units.2._norm # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n",
      "  %1606 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::select(%tempo, %1529, %1518), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %1607 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::copy_(%1606, %1605, %1523), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %_norm.11 : __torch__.torch.nn.modules.normalization.___torch_mangle_32.LayerNorm = prim::GetAttr[name=\"_norm\"](%_3)\n",
      "  %_evaluation : __torch__.torch.nn.modules.linear.___torch_mangle_31.Linear = prim::GetAttr[name=\"_evaluation\"](%_3)\n",
      "  %_memory : __torch__.fgi.units.memory.___torch_mangle_30.SoftMemoryUnit = prim::GetAttr[name=\"_memory\"](%_3)\n",
      "  %_patterns : Tensor = prim::GetAttr[name=\"_patterns\"](%_memory)\n",
      "  %_beta : Tensor = prim::GetAttr[name=\"_beta\"](%_memory)\n",
      "  %1613 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::mul(%_beta, %x.27), scope: __module._property/__module._property._units.3/__module._property._units.3._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %1614 : Float(128, 128, strides=[1, 128], requires_grad=1, device=cpu) = aten::numpy_T(%_patterns), scope: __module._property/__module._property._units.3/__module._property._units.3._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %weighted.25 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::matmul(%1613, %1614), scope: __module._property/__module._property._units.3/__module._property._units.3._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:35:0\n",
      "  %weighted.27 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::softmax(%weighted.25, %1528, %1525), scope: __module._property/__module._property._units.3/__module._property._units.3._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:36:0\n",
      "  %input.41 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::matmul(%weighted.27, %_patterns), scope: __module._property/__module._property._units.3/__module._property._units.3._memory # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\memory.py:37:0\n",
      "  %p : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::dropout(%input.41, %1519, %1523), scope: __module._property/__module._property._units.3/__module._property._units.3._memory/__module._property._units.3._memory._drop # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n",
      "  %bias.47 : Tensor = prim::GetAttr[name=\"bias\"](%_evaluation)\n",
      "  %weight.47 : Tensor = prim::GetAttr[name=\"weight\"](%_evaluation)\n",
      "  %z.13 : Float(1, 1, strides=[1, 1], requires_grad=1, device=cpu) = aten::linear(%x.27, %weight.47, %bias.47), scope: __module._property/__module._property._units.3/__module._property._units.3._evaluation # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n",
      "  %z : Float(1, 1, strides=[1, 1], requires_grad=1, device=cpu) = aten::sigmoid(%z.13), scope: __module._property/__module._property._units.3 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\property.py:22:0\n",
      "  %input.43 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::mul(%p, %z), scope: __module._property/__module._property._units.3 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\property.py:25:0\n",
      "  %bias.49 : Tensor = prim::GetAttr[name=\"bias\"](%_norm.11)\n",
      "  %weight.49 : Tensor = prim::GetAttr[name=\"weight\"](%_norm.11)\n",
      "  %1626 : int[] = prim::ListConstruct(%1522), scope: __module._property/__module._property._units.3/__module._property._units.3._norm\n",
      "  %1627 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::layer_norm(%input.43, %1626, %weight.49, %bias.49, %1521, %1520), scope: __module._property/__module._property._units.3/__module._property._units.3._norm # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n",
      "  %1628 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::select(%tempo, %1529, %1517), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %1629 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::copy_(%1628, %1627, %1523), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:30:0\n",
      "  %x.29 : Float(1, 4, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::transpose(%tempo, %1529, %1528), scope: __module._property # e:\\simulations\\implementations\\notebooks\\..\\fgi\\layer\\ForwardLayer.py:32:0\n",
      "  %1631 : int = prim::Constant[value=1](), scope: __module._co_property/__module._co_property._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\co_property.py:40:0\n",
      "  %1632 : bool = prim::Constant[value=1](), scope: __module._co_property/__module._co_property._units.0/__module._co_property._units.0._norm # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n",
      "  %1633 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module._co_property/__module._co_property._units.0/__module._co_property._units.0._norm # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n",
      "  %1634 : int = prim::Constant[value=128](), scope: __module._co_property/__module._co_property._units.0/__module._co_property._units.0._norm # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n",
      "  %1635 : int = prim::Constant[value=0](), scope: __module._co_property/__module._co_property._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\co_property.py:44:0\n",
      "  %1636 : NoneType = prim::Constant(), scope: __module._co_property/__module._co_property._units.0\n",
      "  %_units : __torch__.torch.nn.modules.container.___torch_mangle_40.ModuleList = prim::GetAttr[name=\"_units\"](%_co_property)\n",
      "  %_0 : __torch__.fgi.units.co_property.ChooseOptions = prim::GetAttr[name=\"0\"](%_units)\n",
      "  %_decides : __torch__.torch.nn.modules.linear.___torch_mangle_39.Linear = prim::GetAttr[name=\"_decides\"](%_0)\n",
      "  %_enhance : __torch__.torch.nn.modules.container.___torch_mangle_38.Sequential = prim::GetAttr[name=\"_enhance\"](%_0)\n",
      "  %_weighted : Tensor = prim::GetAttr[name=\"_weighted\"](%_0)\n",
      "  %_norm : __torch__.torch.nn.modules.normalization.___torch_mangle_35.LayerNorm = prim::GetAttr[name=\"_norm\"](%_0)\n",
      "  %_position : Tensor = prim::GetAttr[name=\"_position\"](%_0)\n",
      "  %input.45 : Float(1, 4, 128, strides=[128, 128, 1], requires_grad=1, device=cpu) = aten::add(%x.29, %_position, %1631), scope: __module._co_property/__module._co_property._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\co_property.py:40:0\n",
      "  %bias.51 : Tensor = prim::GetAttr[name=\"bias\"](%_norm)\n",
      "  %weight.51 : Tensor = prim::GetAttr[name=\"weight\"](%_norm)\n",
      "  %1647 : int[] = prim::ListConstruct(%1634), scope: __module._co_property/__module._co_property._units.0/__module._co_property._units.0._norm\n",
      "  %x.31 : Float(1, 4, 128, strides=[512, 128, 1], requires_grad=1, device=cpu) = aten::layer_norm(%input.45, %1647, %weight.51, %bias.51, %1633, %1632), scope: __module._co_property/__module._co_property._units.0/__module._co_property._units.0._norm # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n",
      "  %weighted : Float(4, strides=[1], requires_grad=1, device=cpu) = aten::softmax(%_weighted, %1635, %1636), scope: __module._co_property/__module._co_property._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\co_property.py:44:0\n",
      "  %input.47 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::matmul(%weighted, %x.31), scope: __module._co_property/__module._co_property._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\co_property.py:45:0\n",
      "  %_1 : __torch__.torch.nn.modules.activation.___torch_mangle_37.ReLU = prim::GetAttr[name=\"1\"](%_enhance)\n",
      "  %_0.11 : __torch__.torch.nn.modules.linear.___torch_mangle_36.Linear = prim::GetAttr[name=\"0\"](%_enhance)\n",
      "  %bias.53 : Tensor = prim::GetAttr[name=\"bias\"](%_0.11)\n",
      "  %weight.53 : Tensor = prim::GetAttr[name=\"weight\"](%_0.11)\n",
      "  %input.49 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::linear(%input.47, %weight.53, %bias.53), scope: __module._co_property/__module._co_property._units.0/__module._co_property._units.0._enhance/__module._co_property._units.0._enhance.0 # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n",
      "  %input : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = aten::relu(%input.49), scope: __module._co_property/__module._co_property._units.0/__module._co_property._units.0._enhance/__module._co_property._units.0._enhance.1 # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\functional.py:1704:0\n",
      "  %bias : Tensor = prim::GetAttr[name=\"bias\"](%_decides)\n",
      "  %weight : Tensor = prim::GetAttr[name=\"weight\"](%_decides)\n",
      "  %x : Float(1, 10, strides=[10, 1], requires_grad=1, device=cpu) = aten::linear(%input, %weight, %bias), scope: __module._co_property/__module._co_property._units.0/__module._co_property._units.0._decides # e:\\simulations\\implementations\\env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n",
      "  %1660 : Float(1, 10, strides=[10, 1], requires_grad=1, device=cpu) = aten::softmax(%x, %1631, %1636), scope: __module._co_property/__module._co_property._units.0 # e:\\simulations\\implementations\\notebooks\\..\\fgi\\units\\co_property.py:74:0\n",
      "  %1003 : (Float(1, 10, strides=[10, 1], requires_grad=1, device=cpu)) = prim::TupleConstruct(%1660)\n",
      "  return (%1003)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(\"runs/experiment\")\n",
    "writer.add_graph(solver, randn((1, 1, 28, 28)), verbose=True)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f29a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
